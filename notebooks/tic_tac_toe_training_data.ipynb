{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create an exhaustive training data set for tic tac toe using the Minimax agent, in the form of a replay buffer compatible with AlphaZeroTrainer. The idea is to use this dataset to run some sweeps, and to understand which deep learning models will perform best in theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applications.tic_tac_toe.game_state import TicTacToeState\n",
    "from core.implementations.Minimax import Minimax\n",
    "\n",
    "# Creat minmax agent and expand the game tree\n",
    "state = TicTacToeState()\n",
    "agent = Minimax(state)\n",
    "agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique examples, translated into AlphaZero format for compatibility with models\n",
    "\n",
    "from core.implementations.AlphaZero import AlphaZeroTarget\n",
    "from core.data_structures import TrainingExample\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def get_subexamples(root) -> Dict[TicTacToeState, TrainingExample[Tuple[int, int], AlphaZeroTarget]]:\n",
    "    policy = {action: 0.0 for action in root.children.keys()}\n",
    "    for action in root.value.best_actions:\n",
    "        policy[action] = 1/len(root.value.best_actions)\n",
    "    state_to_examples = {}\n",
    "    state_to_examples[root.state] = TrainingExample(\n",
    "        state=root.state,\n",
    "        target=(policy, root.value.value),\n",
    "        data={'legal_actions': list(root.children.keys())}\n",
    "    )\n",
    "    for child in root.children.values():\n",
    "        state_to_examples.update(get_subexamples(child))\n",
    "    return state_to_examples\n",
    "\n",
    "examples = list(get_subexamples(agent.root).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "State: \n",
      "X |  | O\n",
      "---------\n",
      " | O | X\n",
      "---------\n",
      "X | O | \n",
      "Target: ({(0, 1): 0.0, (1, 0): 1.0, (2, 2): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 1), (1, 0), (2, 2)]}\n",
      "\n",
      "\n",
      "Example 2:\n",
      "State: \n",
      "X |  | O\n",
      "---------\n",
      " | O | X\n",
      "---------\n",
      "X | O | X\n",
      "Target: ({(0, 1): 1.0, (1, 0): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 1), (1, 0)]}\n",
      "\n",
      "\n",
      "Example 3:\n",
      "State: \n",
      "X |  | O\n",
      "---------\n",
      " | O | X\n",
      "---------\n",
      "X |  | O\n",
      "Target: ({(0, 1): 0.0, (1, 0): 1.0, (2, 1): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 1), (1, 0), (2, 1)]}\n",
      "\n",
      "\n",
      "Number of unique examples: 5478\n"
     ]
    }
   ],
   "source": [
    "k = 1053\n",
    "for i, example in enumerate(examples[k:k+3]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"State: \\n{example.state}\")\n",
    "    print(f\"Target: {example.target}\")\n",
    "    print(f\"Data: {example.data}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Number of unique examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_structures import ReplayBuffer\n",
    "from applications.tic_tac_toe.mlp_model import TicTacToeModelInterface\n",
    "from applications.tic_tac_toe.transformer_model import TicTacToeTransformerInterface\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps') # Change to 'cuda' or 'cpu' if needed\n",
    "\n",
    "for model_name, interface in [\n",
    "    ('mlp', TicTacToeModelInterface),\n",
    "    ('transformer', TicTacToeTransformerInterface)\n",
    "]:\n",
    "    state_encoder = lambda state: interface.encode_state(state, device)\n",
    "    example_encoder = lambda example: interface.encode_example(example, device)\n",
    "    buffer = ReplayBuffer(max_size=len(examples))\n",
    "    buffer.extend(examples, state_encoder, example_encoder)\n",
    "    buffer.save(f'applications/tic_tac_toe/training_data/{model_name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots-and-boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
