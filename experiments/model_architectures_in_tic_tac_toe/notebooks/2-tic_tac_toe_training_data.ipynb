{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/eohjelle/Documents/2025-mcts-playground/mcts-playground\n"
     ]
    }
   ],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('../../..')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create an exhaustive training data set for tic tac toe using the Minimax agent, in the form of a replay buffer compatible with AlphaZeroTrainer. The idea is to use this dataset to run some sweeps, and to understand which deep learning models will perform best in theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.games.tic_tac_toe import TicTacToeState\n",
    "from core.algorithms import Minimax\n",
    "\n",
    "# Creat minmax agent and expand the game tree\n",
    "state = TicTacToeState()\n",
    "agent = Minimax(state)\n",
    "agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 549946\n",
      "Number of states: 5478\n"
     ]
    }
   ],
   "source": [
    "# This is for testing the state_dict design\n",
    "\n",
    "def count_nodes(root):\n",
    "    if root.is_leaf():\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + sum(count_nodes(child) for child in root.children.values())\n",
    "\n",
    "print(f\"Number of nodes: {count_nodes(agent.root)}\")\n",
    "\n",
    "def count_states(agent):\n",
    "    return len(agent.state_dict)\n",
    "\n",
    "print(f\"Number of states: {count_states(agent)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique examples, translated into AlphaZero format for compatibility with models\n",
    "\n",
    "from core.data_structures import TrainingExample\n",
    "\n",
    "def example(state, node):\n",
    "    policy = {action: 0.0 for action in node.children.keys()}\n",
    "    for action in node.value.best_actions:\n",
    "        policy[action] = 1/len(node.value.best_actions)\n",
    "    return TrainingExample(\n",
    "        state=state,\n",
    "        target=(policy, node.value.value),\n",
    "        extra_data={'legal_actions': list(node.children.keys())}\n",
    "    )\n",
    "\n",
    "examples = [example(state, node) for state, node in agent.state_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | \n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 2): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 2)]}\n",
      "\n",
      "\n",
      "Example 2:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      " | X | O\n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 0): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 0)]}\n",
      "\n",
      "\n",
      "Example 3:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | X\n",
      "Target: ({(0, 0): 0.5, (1, 2): 0.5}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2)]}\n",
      "\n",
      "\n",
      "Number of unique examples: 5478\n"
     ]
    }
   ],
   "source": [
    "k = 2836\n",
    "for i, example in enumerate(examples[k:k+3]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"State: \\n{example.state}\")\n",
    "    print(f\"Target: {example.target}\")\n",
    "    print(f\"Data: {example.extra_data}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Number of unique examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 0])\n",
      "Tokenized policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True, False, False,  True])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
      "MLP policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False, False, False,  True,  True,  True,  True,  True])\n",
      "\n",
      "Example 2:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 0, 1, 2])\n",
      "Tokenized policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True,  True, False, False])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "MLP policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False, False,  True,  True,  True,  True,  True, False])\n",
      "\n",
      "Example 3:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 1])\n",
      "Tokenized policy: tensor([0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True, False, False, False])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
      "MLP policy: tensor([0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False,  True,  True,  True,  True,  True, False, False])\n"
     ]
    }
   ],
   "source": [
    "# Test encoding and decoding\n",
    "\n",
    "from experiments.experimenting_with_model_architectures_in_tic_tac_toe.tensor_mapping import MLPTensorMapping, TokenizedTensorMapping\n",
    "import torch\n",
    "\n",
    "tokenized_states, tokenized_targets, tokenized_data = TokenizedTensorMapping.encode_examples(examples, device=torch.device('cpu'))\n",
    "mlp_states, mlp_targets, mlp_data = MLPTensorMapping.encode_examples(examples, device=torch.device('cpu'))\n",
    "\n",
    "for i in range(k, k+3):\n",
    "    print(f\"\\nExample {i+1-k}:\")\n",
    "    print(f\"Tokenized state: {tokenized_states[i]}\")\n",
    "    print(f\"Tokenized policy: {tokenized_targets['policy'][i]}\")\n",
    "    print(f\"Tokenized value: {tokenized_targets['value'][i]}\")\n",
    "    print(f\"Tokenized legal actions: {tokenized_data['legal_actions'][i]}\")\n",
    "    print(f\"MLP state: {mlp_states[i]}\")\n",
    "    print(f\"MLP policy: {mlp_targets['policy'][i]}\")\n",
    "    print(f\"MLP value: {mlp_targets['value'][i]}\")\n",
    "    print(f\"MLP legal actions: {mlp_data['legal_actions'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample entropies: tensor([2.1972, -0.0000, 1.3863, -0.0000, 1.3863, 1.3863, 1.3863, -0.0000, 1.3863,\n",
      "        -0.0000])\n",
      "Average entropy: 0.40638184547424316\n",
      "KL divergence between target and random logits: 1.742671251296997\n",
      "KL divergence between target and uniform logits: 1.406589150428772\n"
     ]
    }
   ],
   "source": [
    "# Check entropy of target distribution\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "targets = tokenized_targets\n",
    "\n",
    "non_zero_policy = torch.where(targets['policy'] > 0, targets['policy'], torch.ones_like(targets['policy']))\n",
    "logits = torch.log(non_zero_policy)\n",
    "entropy = - torch.sum(targets['policy'] * logits, dim=1)\n",
    "\n",
    "print(f\"Some sample entropies: {entropy[:10]}\")\n",
    "print(f\"Average entropy: {entropy.mean()}\")\n",
    "\n",
    "# Compare with entropy of random logits\n",
    "random_logits = torch.randn_like(targets['policy'])\n",
    "random_cross_entropy = F.cross_entropy(random_logits, targets['policy'])\n",
    "\n",
    "print(f\"KL divergence between target and random logits: {random_cross_entropy - entropy.mean()}\")\n",
    "\n",
    "# Compare with entropy of uniform logits\n",
    "uniform_logits = torch.zeros_like(targets['policy'])\n",
    "uniform_cross_entropy = F.cross_entropy(uniform_logits, targets['policy'])\n",
    "\n",
    "print(f\"KL divergence between target and uniform logits: {uniform_cross_entropy - entropy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_structures import ReplayBuffer\n",
    "\n",
    "for tensor_mapping, states, targets, data in zip([MLPTensorMapping, TokenizedTensorMapping], [mlp_states, tokenized_states], [mlp_targets, tokenized_targets], [mlp_data, tokenized_data]):\n",
    "    buffer = ReplayBuffer(max_size=len(examples))\n",
    "    buffer.add(states, targets, data)\n",
    "    artifact_name = f'buffer_minimax_{tensor_mapping.__name__}'\n",
    "    path = f'experiments/experimenting_with_model_architectures_in_tic_tac_toe/data/{artifact_name}.pt'\n",
    "    buffer.save_to_file(path)\n",
    "# buffer.save_to_wandb(\n",
    "#     artifact_name=artifact_name,\n",
    "#     project='AlphaZero-TicTacToe',\n",
    "#     description=f'Training data for tic tac toe and {tensor_mapping.__name__} tensor mapping created by Minimax agent'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
