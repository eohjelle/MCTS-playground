{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes\n"
     ]
    }
   ],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create an exhaustive training data set for tic tac toe using the Minimax agent, in the form of a replay buffer compatible with AlphaZeroTrainer. The idea is to use this dataset to run some sweeps, and to understand which deep learning models will perform best in theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from applications.tic_tac_toe.game_state import TicTacToeState\n",
    "from core.implementations.Minimax import Minimax\n",
    "\n",
    "# Creat minmax agent and expand the game tree\n",
    "state = TicTacToeState()\n",
    "agent = Minimax(state)\n",
    "agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 549946\n",
      "Number of states: 5478\n"
     ]
    }
   ],
   "source": [
    "# This is for testing the state_dict design\n",
    "\n",
    "def count_nodes(root):\n",
    "    if root.is_leaf():\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + sum(count_nodes(child) for child in root.children.values())\n",
    "\n",
    "print(f\"Number of nodes: {count_nodes(agent.root)}\")\n",
    "\n",
    "def count_states(agent):\n",
    "    return len(agent.state_dict)\n",
    "\n",
    "print(f\"Number of states: {count_states(agent)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique examples, translated into AlphaZero format for compatibility with models\n",
    "\n",
    "from core.data_structures import TrainingExample\n",
    "\n",
    "def example(state, node):\n",
    "    policy = {action: 0.0 for action in node.children.keys()}\n",
    "    for action in node.value.best_actions:\n",
    "        policy[action] = 1/len(node.value.best_actions)\n",
    "    return TrainingExample(\n",
    "        state=state,\n",
    "        target=(policy, node.value.value),\n",
    "        data={'legal_actions': list(node.children.keys())}\n",
    "    )\n",
    "\n",
    "examples = [example(state, node) for state, node in agent.state_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | \n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 2): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 2)]}\n",
      "\n",
      "\n",
      "Example 2:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      " | X | O\n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 0): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 0)]}\n",
      "\n",
      "\n",
      "Example 3:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | X\n",
      "Target: ({(0, 0): 0.5, (1, 2): 0.5}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2)]}\n",
      "\n",
      "\n",
      "Number of unique examples: 5478\n"
     ]
    }
   ],
   "source": [
    "k = 2836\n",
    "for i, example in enumerate(examples[k:k+3]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"State: \\n{example.state}\")\n",
    "    print(f\"Target: {example.target}\")\n",
    "    print(f\"Data: {example.data}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Number of unique examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Encoded state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 0], device='mps:0')\n",
      "Encoded policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.], device='mps:0')\n",
      "Encoded value: 1.0\n",
      "Encoded legal actions: tensor([1., 0., 0., 0., 0., 1., 0., 0., 1.], device='mps:0')\n",
      "\n",
      "Example 2:\n",
      "Encoded state: tensor([0, 1, 1, 2, 2, 0, 0, 1, 2], device='mps:0')\n",
      "Encoded policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.], device='mps:0')\n",
      "Encoded value: 1.0\n",
      "Encoded legal actions: tensor([1., 0., 0., 0., 0., 1., 1., 0., 0.], device='mps:0')\n",
      "\n",
      "Example 3:\n",
      "Encoded state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 1], device='mps:0')\n",
      "Encoded policy: tensor([0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "       device='mps:0')\n",
      "Encoded value: 1.0\n",
      "Encoded legal actions: tensor([1., 0., 0., 0., 0., 1., 0., 0., 0.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Test encoding and decoding\n",
    "\n",
    "from applications.tic_tac_toe.tensor_mapping import TokenizedTensorMapping\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps') # Change to 'cuda' or 'cpu' if needed\n",
    "\n",
    "tensor_mapping = TokenizedTensorMapping\n",
    "states = tensor_mapping.encode_states([ex.state for ex in examples], device) # type: ignore\n",
    "targets, data = tensor_mapping.encode_examples(examples, device)\n",
    "\n",
    "for i in range(k, k+3):\n",
    "    print(f\"\\nExample {i+1-k}:\")\n",
    "    print(f\"Encoded state: {states[i]}\")\n",
    "    print(f\"Encoded policy: {targets['policy'][i]}\")\n",
    "    print(f\"Encoded value: {targets['value'][i]}\")\n",
    "    print(f\"Encoded legal actions: {data['legal_actions'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample entropies: tensor([2.1972, -0.0000, 1.3863, -0.0000, 1.3863, 1.3863, 1.3863, -0.0000, 1.3863,\n",
      "        -0.0000], device='mps:0')\n",
      "Average entropy: 0.40638184547424316\n",
      "Average entropy of random logits: 2.154865264892578\n"
     ]
    }
   ],
   "source": [
    "# Check entropy of target distribution\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "non_zero_policy = torch.where(targets['policy'] > 0, targets['policy'], torch.ones_like(targets['policy']))\n",
    "logits = torch.log(non_zero_policy)\n",
    "entropy = - torch.sum(targets['policy'] * logits, dim=1)\n",
    "\n",
    "print(f\"Some sample entropies: {entropy[:10]}\")\n",
    "print(f\"Average entropy: {entropy.mean()}\")\n",
    "\n",
    "# Compare with entropy of random logits\n",
    "random_logits = torch.randn_like(targets['policy'])\n",
    "random_entropy = F.cross_entropy(random_logits, targets['policy'])\n",
    "\n",
    "print(f\"Average entropy of random logits: {random_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory applications/tic_tac_toe/training_data does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m artifact_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtic_tac_toe_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_mapping\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_minimax\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplications/tic_tac_toe/training_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# buffer.save_to_wandb(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     artifact_name=artifact_name,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     project='AlphaZero-TicTacToe',\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     description=f'Training data for tic tac toe and {tensor_mapping.__name__} tensor mapping created by Minimax agent'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/data_structures.py:62\u001b[0m, in \u001b[0;36mReplayBuffer.save\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save replay buffer to disk.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        path: Path to save the replay buffer to\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dots-and-boxes/lib/python3.13/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dots-and-boxes/lib/python3.13/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dots-and-boxes/lib/python3.13/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory applications/tic_tac_toe/training_data does not exist."
     ]
    }
   ],
   "source": [
    "from core.data_structures import ReplayBuffer\n",
    "\n",
    "buffer = ReplayBuffer(max_size=len(examples))\n",
    "buffer.extend(states, targets, data)\n",
    "artifact_name = f'tic_tac_toe_{tensor_mapping.__name__}_minimax'\n",
    "path = f'applications/tic_tac_toe/training_data/{artifact_name}.pkl'\n",
    "buffer.save(path)\n",
    "# buffer.save_to_wandb(\n",
    "#     artifact_name=artifact_name,\n",
    "#     project='AlphaZero-TicTacToe',\n",
    "#     description=f'Training data for tic tac toe and {tensor_mapping.__name__} tensor mapping created by Minimax agent'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots-and-boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
