{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eohjelle/Documents/2025-mcts-playground/mcts-playground\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import ReplayBuffer\n",
    "from experiments.connect_four.tensor_mapping import ConnectFourTensorMapping\n",
    "\n",
    "buffer = ReplayBuffer.from_file(\"checkpoints/connect_four/ResMLP:5-32-128:fixed-rewards/buffer.pt\")\n",
    "\n",
    "TM = ConnectFourTensorMapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states, targets, extra_data = buffer.states, buffer.targets, buffer.extra_data\n",
    "\n",
    "\n",
    "# print(f\"State shape: {states.shape}\")\n",
    "# print(f\"Target keys: {targets.keys()}\")\n",
    "# print(f\"Extra data keys: {extra_data.keys()}\")\n",
    "\n",
    "# tuples = list(zip(states, targets['policy'], targets['value'], extra_data['legal_actions']))\n",
    "\n",
    "# start = 17940\n",
    "# for i, (state, policy, value, legal_actions) in enumerate(tuples[start:17946]):\n",
    "#     print(f\"Index: {i + start}\")\n",
    "#     print(f\"State: {state}\")\n",
    "#     print(f\"Policy: {policy}\")\n",
    "#     print(f\"Value: {value}\")\n",
    "#     print(f\"Legal actions: {legal_actions}\")\n",
    "#     print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# policy_nans, value_nans, policy_invalids, legal_actions_nans = [], [], [], []\n",
    "\n",
    "# # Find and examine first few NaN policies\n",
    "# for i, (state, policy, value, legal_actions) in enumerate(tuples[:17946]):\n",
    "#     show = False\n",
    "#     if torch.isnan(policy).any():\n",
    "#         print(f\"Found NaN policy at index {i}\")\n",
    "#         policy_nans.append(i)\n",
    "#         show = True\n",
    "#     if not torch.isclose(policy.sum(), torch.tensor(1.0)):\n",
    "#         print(f\"Found invalid policy at index {i}\") \n",
    "#         policy_invalids.append(i)\n",
    "#         show = True\n",
    "#     if torch.isnan(value):\n",
    "#         print(f\"Found NaN value at index {i}\")\n",
    "#         value_nans.append(i)\n",
    "#         show = True\n",
    "#     if torch.isnan(legal_actions).any():\n",
    "#         print(f\"Found NaN legal actions at index {i}\")\n",
    "#         legal_actions_nans.append(i)\n",
    "#         show = True\n",
    "#     if show:\n",
    "#         print(f\"Index {i}:\")\n",
    "#         print(f\"State shape: {state.shape}\")\n",
    "#         print(f\"Policy: {policy}\")\n",
    "#         print(f\"Value: {value}\")\n",
    "#         print(f\"Legal actions: {legal_actions}\")\n",
    "#         print(\"=\" * 80)\n",
    "\n",
    "# print(f\"Policy nans: {len(policy_nans)}\")\n",
    "# print(f\"Value nans: {len(value_nans)}\")\n",
    "# print(f\"Policy invalids: {len(policy_invalids)}\")\n",
    "# print(f\"Legal actions nans: {len(legal_actions_nans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# policy_zeros, value_nans, policy_invalids, legal_actions_nans = [], [], [], []\n",
    "\n",
    "# # Find and examine first few NaN policies\n",
    "# for i, (state, policy, value, legal_actions) in enumerate(tuples[:17946]):\n",
    "#     show = False\n",
    "#     for j in range(policy.shape[0]):\n",
    "#         if torch.isclose(policy[j], torch.tensor(0.0)):\n",
    "#             # print(f\"Found zero value at index {i}, column {j}\")\n",
    "#             policy_zeros.append(i)\n",
    "#             show = True\n",
    "#             break\n",
    "#     if torch.isnan(value):\n",
    "#         # print(f\"Found NaN value at index {i}\")\n",
    "#         value_nans.append(i)\n",
    "#         show = True\n",
    "#     # if show:\n",
    "#     #     print(f\"Index {i}:\")\n",
    "#     #     print(f\"Policy: {policy}\")\n",
    "#     #     print(f\"Value: {value}\")\n",
    "#     #     print(\"=\" * 80)\n",
    "\n",
    "# print(f\"Policy zeros: {len(policy_zeros)}\")\n",
    "# print(f\"Value nans: {len(value_nans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "=== STEP 1: Generate Trajectories ===\n",
      "Generated 6 trajectories\n",
      "Trajectory 0 final reward: -1.0\n",
      "✅ Trajectory 0 final reward is valid\n",
      "Trajectory 1 final reward: 1.0\n",
      "✅ Trajectory 1 final reward is valid\n",
      "Trajectory 2 final reward: -1.0\n",
      "✅ Trajectory 2 final reward is valid\n",
      "Trajectory 3 final reward: 1.0\n",
      "✅ Trajectory 3 final reward is valid\n",
      "Trajectory 4 final reward: -1.0\n",
      "✅ Trajectory 4 final reward is valid\n",
      "Trajectory 5 final reward: 1.0\n",
      "✅ Trajectory 5 final reward is valid\n",
      "\n",
      "=== STEP 2: Extract Examples ===\n",
      "Extracted 8 examples from trajectory 0\n",
      "Extracted 8 examples from trajectory 1\n",
      "Extracted 15 examples from trajectory 2\n",
      "Extracted 15 examples from trajectory 3\n",
      "Extracted 11 examples from trajectory 4\n",
      "Extracted 11 examples from trajectory 5\n",
      "✅ Total examples extracted: 68\n",
      "\n",
      "=== STEP 3: Encode Examples with Tensor Mapping ===\n",
      "Encoded states shape: torch.Size([68, 84])\n",
      "Target keys: dict_keys(['policy', 'value'])\n",
      "✅ Encoded states are valid\n",
      "✅ Policy targets are valid\n",
      "✅ Value targets are valid\n",
      "Legal actions shape: torch.Size([68, 7])\n",
      "✅ Legal actions are valid\n",
      "\n",
      "=== STEP 4: Model Forward Pass ===\n",
      "Model output keys: dict_keys(['policy', 'value'])\n",
      "✅ Model output 'policy' is valid\n",
      "  Shape: torch.Size([8, 7]), Range: [-0.9896, 0.9585]\n",
      "✅ Model output 'value' is valid\n",
      "  Shape: torch.Size([8]), Range: [-0.4003, 0.5430]\n",
      "\n",
      "=== STEP 5: Compute Loss ===\n",
      "Total loss: 1.8807101249694824\n",
      "policy_loss: 0.43846791982650757\n",
      "value_loss: 1.44224214553833\n",
      "✅ Loss is valid\n",
      "\n",
      "=== STEP 6: Detailed State Analysis ===\n",
      "Analyzing some example states...\n",
      "\n",
      "--- Example 0 ---\n",
      "Is terminal: False\n",
      "Current player: 0\n",
      "Legal actions: [0, 1, 2, 3, 4, 5, 6]\n",
      "Rewards: <bound method OpenSpielState.rewards of <core.games.open_spiel_state_wrapper.OpenSpielState object at 0x11d079e50>>\n",
      "Policy target: {0: 0.68, 1: 0.02, 2: 0.02, 3: 0.02, 4: 0.02, 5: 0.04, 6: 0.2}\n",
      "Value target: -1.0\n",
      "❌ Error in state analysis: 'function' object has no attribute 'items'\n",
      "\n",
      "=== Summary ===\n",
      "Check the output above for any ❌ markers indicating NaN values.\n",
      "Pay special attention to:\n",
      "1. Final trajectory rewards\n",
      "2. Value targets in examples\n",
      "3. Model outputs (especially if they produce extreme values)\n",
      "4. Loss computation components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/22/9ys3dbcs5yb0wn1j_xcbb4600000gn/T/ipykernel_39691/3483481202.py\", line 289, in <module>\n",
      "    for player, reward in state.rewards.items():\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'function' object has no attribute 'items'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyspiel\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "# Import your modules\n",
    "from core.games.open_spiel_state_wrapper import OpenSpielState\n",
    "from core.algorithms.AlphaZero import AlphaZero, AlphaZeroConfig\n",
    "from core.algorithms.MCTS import MCTS, MCTSConfig\n",
    "from core.model_interface import Model, ModelPredictor\n",
    "from core.simulation import generate_trajectories\n",
    "from experiments.connect_four.tensor_mapping import ConnectFourTensorMapping\n",
    "from experiments.connect_four.models.resmlp import ResMLP, ResMLPInitParams\n",
    "from core.algorithms.AlphaZero import AlphaZeroTrainingAdapter\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create initial state function\n",
    "def create_initial_state():\n",
    "    game = pyspiel.load_game(\"connect_four\")\n",
    "    return OpenSpielState(game.new_initial_state(), num_players=2)\n",
    "\n",
    "# Initialize model (adjust parameters as needed)\n",
    "model_params: ResMLPInitParams = ResMLPInitParams(\n",
    "    input_dim=84,  # 2 * 6 * 7 = 84 for Connect Four\n",
    "    num_residual_blocks=2,\n",
    "    residual_dim=32,\n",
    "    hidden_size=128,\n",
    "    policy_head_dim=7\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    model_architecture=ResMLP,\n",
    "    init_params=model_params,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create tensor mapping and training adapter\n",
    "tensor_mapping = ConnectFourTensorMapping()\n",
    "training_adapter = AlphaZeroTrainingAdapter()\n",
    "\n",
    "# Create model predictor\n",
    "model_predictor = ModelPredictor(model, tensor_mapping)\n",
    "\n",
    "# Create AlphaZero agent\n",
    "alphazero_config = AlphaZeroConfig(\n",
    "    num_simulations=50,  # Small number for testing\n",
    ")\n",
    "\n",
    "print(\"=== STEP 1: Generate Trajectories ===\")\n",
    "try:\n",
    "    trajectories = generate_trajectories(\n",
    "        initial_state_creator=create_initial_state,\n",
    "        trajectory_player_creators=[\n",
    "            lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config),\n",
    "            lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config)\n",
    "        ],\n",
    "        opponent_creators=[],\n",
    "        num_games=3,  # Generate a few games\n",
    "        logger=logging.getLogger(__name__)\n",
    "    )\n",
    "    print(f\"Generated {len(trajectories)} trajectories\")\n",
    "    \n",
    "    # Check trajectory outcomes for NaN\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        final_reward = trajectory[-1].reward\n",
    "        print(f\"Trajectory {i} final reward: {final_reward}\")\n",
    "        if np.isnan(final_reward):\n",
    "            print(f\"❌ NaN found in trajectory {i} final reward!\")\n",
    "        else:\n",
    "            print(f\"✅ Trajectory {i} final reward is valid\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error generating trajectories: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 2: Extract Examples ===\")\n",
    "examples = []\n",
    "try:\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        traj_examples = training_adapter.extract_examples(trajectory)\n",
    "        examples.extend(traj_examples)\n",
    "        print(f\"Extracted {len(traj_examples)} examples from trajectory {i}\")\n",
    "        \n",
    "        # Check each example for NaN values\n",
    "        for j, example in enumerate(traj_examples):\n",
    "            policy_target, value_target = example.target\n",
    "            \n",
    "            # Check policy target\n",
    "            policy_values = list(policy_target.values())\n",
    "            if any(np.isnan(val) for val in policy_values):\n",
    "                print(f\"❌ NaN found in policy target at trajectory {i}, example {j}\")\n",
    "                print(f\"Policy: {policy_target}\")\n",
    "            \n",
    "            # Check value target\n",
    "            if np.isnan(value_target):\n",
    "                print(f\"❌ NaN found in value target at trajectory {i}, example {j}: {value_target}\")\n",
    "            \n",
    "    print(f\"✅ Total examples extracted: {len(examples)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error extracting examples: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 3: Encode Examples with Tensor Mapping ===\")\n",
    "try:\n",
    "    states, targets, extra_data = tensor_mapping.encode_examples(examples, device)\n",
    "    \n",
    "    print(f\"Encoded states shape: {states.shape}\")\n",
    "    print(f\"Target keys: {targets.keys()}\")\n",
    "    \n",
    "    # Check for NaN in encoded data\n",
    "    if torch.isnan(states).any():\n",
    "        print(\"❌ NaN found in encoded states!\")\n",
    "        nan_indices = torch.isnan(states).nonzero()\n",
    "        print(f\"NaN locations in states: {nan_indices[:10]}...\")  # Show first 10\n",
    "    else:\n",
    "        print(\"✅ Encoded states are valid\")\n",
    "    \n",
    "    # Check policy targets\n",
    "    policy_targets = targets['policy']\n",
    "    if torch.isnan(policy_targets).any():\n",
    "        print(\"❌ NaN found in policy targets!\")\n",
    "        nan_indices = torch.isnan(policy_targets).nonzero()\n",
    "        print(f\"NaN locations in policy targets: {nan_indices[:10]}...\")\n",
    "        \n",
    "        # Print some examples of NaN policies\n",
    "        nan_rows = torch.isnan(policy_targets).any(dim=1).nonzero().flatten()\n",
    "        print(f\"Rows with NaN policies: {nan_rows[:5]}\")\n",
    "        for row in nan_rows[:3]:\n",
    "            print(f\"Row {row}: {policy_targets[row]}\")\n",
    "    else:\n",
    "        print(\"✅ Policy targets are valid\")\n",
    "        \n",
    "    # Check value targets\n",
    "    value_targets = targets['value']\n",
    "    if torch.isnan(value_targets).any():\n",
    "        print(\"❌ NaN found in value targets!\")\n",
    "        nan_indices = torch.isnan(value_targets).nonzero()\n",
    "        print(f\"NaN locations in value targets: {nan_indices[:10]}...\")\n",
    "        print(f\"NaN values: {value_targets[torch.isnan(value_targets)][:10]}\")\n",
    "    else:\n",
    "        print(\"✅ Value targets are valid\")\n",
    "        \n",
    "    # Check extra data\n",
    "    legal_actions = extra_data['legal_actions']\n",
    "    print(f\"Legal actions shape: {legal_actions.shape}\")\n",
    "    if torch.isnan(legal_actions.float()).any():\n",
    "        print(\"❌ NaN found in legal actions!\")\n",
    "    else:\n",
    "        print(\"✅ Legal actions are valid\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error encoding examples: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 4: Model Forward Pass ===\")\n",
    "try:\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Take a small batch to test\n",
    "        batch_size = min(8, states.shape[0])\n",
    "        batch_states = states[:batch_size]\n",
    "        batch_targets = {k: v[:batch_size] for k, v in targets.items()}\n",
    "        batch_extra_data = {k: v[:batch_size] for k, v in extra_data.items()}\n",
    "        \n",
    "        model_outputs = model.model(batch_states)\n",
    "        \n",
    "        print(f\"Model output keys: {model_outputs.keys()}\")\n",
    "        \n",
    "        # Check model outputs for NaN\n",
    "        for key, tensor in model_outputs.items():\n",
    "            if torch.isnan(tensor).any():\n",
    "                print(f\"❌ NaN found in model output '{key}'!\")\n",
    "                print(f\"Shape: {tensor.shape}\")\n",
    "                print(f\"NaN count: {torch.isnan(tensor).sum().item()}\")\n",
    "                nan_indices = torch.isnan(tensor).nonzero()\n",
    "                print(f\"First few NaN locations: {nan_indices[:5]}\")\n",
    "                \n",
    "                # Show the actual values\n",
    "                print(f\"Sample values: {tensor.flatten()[:10]}\")\n",
    "            else:\n",
    "                print(f\"✅ Model output '{key}' is valid\")\n",
    "                print(f\"  Shape: {tensor.shape}, Range: [{tensor.min().item():.4f}, {tensor.max().item():.4f}]\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in model forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 5: Compute Loss ===\")\n",
    "try:\n",
    "    # Put model in training mode for loss computation\n",
    "    model.model.train()\n",
    "    \n",
    "    # Compute loss using the training adapter\n",
    "    loss, metrics = training_adapter.compute_loss(model_outputs, batch_targets, batch_extra_data)\n",
    "    \n",
    "    print(f\"Total loss: {loss.item()}\")\n",
    "    if metrics:\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            print(f\"{metric_name}: {metric_value}\")\n",
    "    \n",
    "    # Check if loss is NaN\n",
    "    if torch.isnan(loss):\n",
    "        print(\"❌ Loss is NaN!\")\n",
    "        \n",
    "        # Let's manually compute the loss components to find where NaN comes from\n",
    "        print(\"\\n--- Manual Loss Debugging ---\")\n",
    "        \n",
    "        policy_logits = model_outputs[\"policy\"]\n",
    "        value_preds = model_outputs[\"value\"]\n",
    "        policy_targets = batch_targets['policy']\n",
    "        value_targets = batch_targets['value']\n",
    "        legal_actions_mask = batch_extra_data['legal_actions']\n",
    "        \n",
    "        print(f\"Policy logits stats: min={policy_logits.min()}, max={policy_logits.max()}, nan_count={torch.isnan(policy_logits).sum()}\")\n",
    "        print(f\"Value preds stats: min={value_preds.min()}, max={value_preds.max()}, nan_count={torch.isnan(value_preds).sum()}\")\n",
    "        print(f\"Policy targets stats: min={policy_targets.min()}, max={policy_targets.max()}, nan_count={torch.isnan(policy_targets).sum()}\")\n",
    "        print(f\"Value targets stats: min={value_targets.min()}, max={value_targets.max()}, nan_count={torch.isnan(value_targets).sum()}\")\n",
    "        \n",
    "        # Policy loss computation\n",
    "        try:\n",
    "            import torch.nn.functional as F\n",
    "            \n",
    "            # Apply legal actions mask\n",
    "            masked_policy_logits = policy_logits.clone()\n",
    "            masked_policy_logits[~legal_actions_mask] = float('-inf')\n",
    "            print(f\"After masking: min={masked_policy_logits.min()}, max={masked_policy_logits.max()}\")\n",
    "            \n",
    "            # Compute log probabilities\n",
    "            log_probs = F.log_softmax(masked_policy_logits, dim=-1)\n",
    "            print(f\"Log probs: min={log_probs.min()}, max={log_probs.max()}, nan_count={torch.isnan(log_probs).sum()}\")\n",
    "            \n",
    "            # Compute policy loss\n",
    "            policy_loss = -torch.sum(policy_targets * log_probs, dim=-1).mean()\n",
    "            print(f\"Policy loss: {policy_loss.item()}, is_nan: {torch.isnan(policy_loss)}\")\n",
    "            \n",
    "            # Compute value loss\n",
    "            value_loss = F.mse_loss(value_preds.squeeze(), value_targets)\n",
    "            print(f\"Value loss: {value_loss.item()}, is_nan: {torch.isnan(value_loss)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in manual loss computation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "    else:\n",
    "        print(\"✅ Loss is valid\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error computing loss: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 6: Detailed State Analysis ===\")\n",
    "# Let's look at some specific states that might be problematic\n",
    "try:\n",
    "    print(\"Analyzing some example states...\")\n",
    "    \n",
    "    for i in range(min(3, len(examples))):\n",
    "        example = examples[i]\n",
    "        state = example.state\n",
    "        policy_target, value_target = example.target\n",
    "        \n",
    "        print(f\"\\n--- Example {i} ---\")\n",
    "        print(f\"Is terminal: {state.is_terminal}\")\n",
    "        print(f\"Current player: {state.current_player}\")\n",
    "        print(f\"Legal actions: {state.legal_actions}\")\n",
    "        print(f\"Rewards: {state.rewards}\")\n",
    "        print(f\"Policy target: {policy_target}\")\n",
    "        print(f\"Value target: {value_target}\")\n",
    "        \n",
    "        # Check for issues\n",
    "        if np.isnan(value_target):\n",
    "            print(\"❌ This example has NaN value target!\")\n",
    "        \n",
    "        # Check if current_player is problematic\n",
    "        if state.current_player not in [0, 1]:\n",
    "            print(f\"❌ Unusual current_player: {state.current_player}\")\n",
    "            \n",
    "        # Check if rewards have issues\n",
    "        for player, reward in state.rewards.items():\n",
    "            if np.isnan(reward):\n",
    "                print(f\"❌ NaN reward for player {player}: {reward}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in state analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(\"Check the output above for any ❌ markers indicating NaN values.\")\n",
    "print(\"Pay special attention to:\")\n",
    "print(\"1. Final trajectory rewards\")\n",
    "print(\"2. Value targets in examples\")\n",
    "print(\"3. Model outputs (especially if they produce extreme values)\")\n",
    "print(\"4. Loss computation components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Policy Confidence vs NaN Values ===\n",
      "Scanning buffer for NaN values and analyzing corresponding policies...\n",
      "Error accessing index 0: 'ReplayBuffer' object is not subscriptable\n",
      "\n",
      "=== Analysis Results ===\n",
      "Found 0 entries with NaN values\n",
      "Collected 0 valid entries for comparison\n",
      "\n",
      "=== Testing Numerical Issues with Confident Policies ===\n",
      "Loss with confident policy target: 1.5551072359085083\n",
      "✅ Confident policy target works fine\n",
      "Loss with moderate policy target: 0.7357421517372131\n",
      "\n",
      "--- Testing Extreme Model Outputs ---\n",
      "Loss with extreme model output: 5.101607799530029\n",
      "✅ Extreme model output handled correctly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== Testing Policy Confidence vs NaN Values ===\")\n",
    "\n",
    "def analyze_policy_confidence(policy_tensor):\n",
    "    \"\"\"Analyze how confident/extreme a policy is\"\"\"\n",
    "    probs = policy_tensor.cpu().numpy()\n",
    "    \n",
    "    max_prob = np.max(probs)\n",
    "    min_prob = np.min(probs)\n",
    "    \n",
    "    # Calculate entropy (lower = more confident)\n",
    "    # Avoid log(0) by adding small epsilon\n",
    "    epsilon = 1e-8\n",
    "    entropy = -np.sum(probs * np.log(probs + epsilon))\n",
    "    \n",
    "    # Count near-zero probabilities\n",
    "    near_zero_count = np.sum(probs < 0.01)\n",
    "    \n",
    "    # Count near-one probabilities  \n",
    "    near_one_count = np.sum(probs > 0.99)\n",
    "    \n",
    "    return {\n",
    "        'max_prob': max_prob,\n",
    "        'min_prob': min_prob,\n",
    "        'entropy': entropy,\n",
    "        'near_zero_count': near_zero_count,\n",
    "        'near_one_count': near_one_count,\n",
    "        'policy': probs\n",
    "    }\n",
    "\n",
    "# Collect data about policies with and without NaN values\n",
    "nan_policies = []\n",
    "valid_policies = []\n",
    "nan_indices = []\n",
    "valid_indices = []\n",
    "\n",
    "print(\"Scanning buffer for NaN values and analyzing corresponding policies...\")\n",
    "\n",
    "# Scan through buffer (adjust range based on your buffer size)\n",
    "for i in range(min(len(buffer), 20000)):  # Scan up to 20k entries\n",
    "    try:\n",
    "        state, targets, extra_data = buffer[i]\n",
    "        policy = targets['policy']\n",
    "        value = targets['value']\n",
    "        \n",
    "        if torch.isnan(value).any():\n",
    "            # Found NaN value - analyze the policy\n",
    "            policy_analysis = analyze_policy_confidence(policy)\n",
    "            nan_policies.append(policy_analysis)\n",
    "            nan_indices.append(i)\n",
    "            \n",
    "            if len(nan_policies) <= 10:  # Print first 10 for inspection\n",
    "                print(f\"\\n❌ NaN Value at index {i}:\")\n",
    "                print(f\"  Policy: {policy_analysis['policy']}\")\n",
    "                print(f\"  Max prob: {policy_analysis['max_prob']:.6f}\")\n",
    "                print(f\"  Min prob: {policy_analysis['min_prob']:.6f}\")\n",
    "                print(f\"  Entropy: {policy_analysis['entropy']:.6f}\")\n",
    "                print(f\"  Near-zero count: {policy_analysis['near_zero_count']}\")\n",
    "                print(f\"  Near-one count: {policy_analysis['near_one_count']}\")\n",
    "        else:\n",
    "            # Valid value - collect some samples for comparison\n",
    "            if len(valid_policies) < 1000:  # Collect up to 1000 valid samples\n",
    "                policy_analysis = analyze_policy_confidence(policy)\n",
    "                valid_policies.append(policy_analysis)\n",
    "                valid_indices.append(i)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing index {i}: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n=== Analysis Results ===\")\n",
    "print(f\"Found {len(nan_policies)} entries with NaN values\")\n",
    "print(f\"Collected {len(valid_policies)} valid entries for comparison\")\n",
    "\n",
    "if len(nan_policies) > 0 and len(valid_policies) > 0:\n",
    "    # Calculate statistics\n",
    "    nan_max_probs = [p['max_prob'] for p in nan_policies]\n",
    "    nan_entropies = [p['entropy'] for p in nan_policies]\n",
    "    nan_near_ones = [p['near_one_count'] for p in nan_policies]\n",
    "    \n",
    "    valid_max_probs = [p['max_prob'] for p in valid_policies]\n",
    "    valid_entropies = [p['entropy'] for p in valid_policies]\n",
    "    valid_near_ones = [p['near_one_count'] for p in valid_policies]\n",
    "    \n",
    "    print(f\"\\n--- Policy Confidence Comparison ---\")\n",
    "    print(f\"NaN Policies:\")\n",
    "    print(f\"  Average max probability: {np.mean(nan_max_probs):.6f} ± {np.std(nan_max_probs):.6f}\")\n",
    "    print(f\"  Average entropy: {np.mean(nan_entropies):.6f} ± {np.std(nan_entropies):.6f}\")\n",
    "    print(f\"  Average near-one count: {np.mean(nan_near_ones):.2f}\")\n",
    "    print(f\"  Policies with prob=1.0: {sum(1 for p in nan_max_probs if p >= 0.999)}\")\n",
    "    \n",
    "    print(f\"\\nValid Policies:\")\n",
    "    print(f\"  Average max probability: {np.mean(valid_max_probs):.6f} ± {np.std(valid_max_probs):.6f}\")\n",
    "    print(f\"  Average entropy: {np.mean(valid_entropies):.6f} ± {np.std(valid_entropies):.6f}\")\n",
    "    print(f\"  Average near-one count: {np.mean(valid_near_ones):.2f}\")\n",
    "    print(f\"  Policies with prob=1.0: {sum(1 for p in valid_max_probs if p >= 0.999)}\")\n",
    "    \n",
    "    # Test hypothesis: Are NaN policies significantly more confident?\n",
    "    if np.mean(nan_max_probs) > np.mean(valid_max_probs) + 0.1:\n",
    "        print(f\"\\n✅ HYPOTHESIS CONFIRMED: NaN policies are significantly more confident!\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Hypothesis not confirmed: No significant difference in policy confidence\")\n",
    "    \n",
    "    # Look for perfect policies (exactly 1.0 somewhere)\n",
    "    perfect_nan_policies = [p for p in nan_policies if p['max_prob'] >= 0.999]\n",
    "    perfect_valid_policies = [p for p in valid_policies if p['max_prob'] >= 0.999]\n",
    "    \n",
    "    print(f\"\\n--- Perfect Policies (prob ≥ 0.999) ---\")\n",
    "    print(f\"NaN entries with perfect policies: {len(perfect_nan_policies)}/{len(nan_policies)} ({100*len(perfect_nan_policies)/len(nan_policies):.1f}%)\")\n",
    "    print(f\"Valid entries with perfect policies: {len(perfect_valid_policies)}/{len(valid_policies)} ({100*len(perfect_valid_policies)/len(valid_policies):.1f}%)\")\n",
    "\n",
    "# Test the numerical issue hypothesis\n",
    "print(f\"\\n=== Testing Numerical Issues with Confident Policies ===\")\n",
    "\n",
    "# Create a very confident policy and see if it causes NaN in loss computation\n",
    "confident_policy = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], device=device).unsqueeze(0)\n",
    "moderate_policy = torch.tensor([0.7, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05], device=device).unsqueeze(0)\n",
    "\n",
    "dummy_state = torch.randn(1, 84, device=device)\n",
    "dummy_extra_data = {'legal_actions': torch.ones(1, 7, dtype=torch.bool, device=device)}\n",
    "\n",
    "# Test with confident policy\n",
    "try:\n",
    "    dummy_targets_confident = {'policy': confident_policy, 'value': torch.tensor([1.0], device=device)}\n",
    "    \n",
    "    # Simulate model output\n",
    "    model_output = {\n",
    "        'policy': torch.randn(1, 7, device=device),  # Random logits\n",
    "        'value': torch.tensor([0.5], device=device)\n",
    "    }\n",
    "    \n",
    "    loss_confident, metrics_confident = training_adapter.compute_loss(\n",
    "        model_output, dummy_targets_confident, dummy_extra_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Loss with confident policy target: {loss_confident.item()}\")\n",
    "    if torch.isnan(loss_confident):\n",
    "        print(\"❌ Confident policy target causes NaN loss!\")\n",
    "    else:\n",
    "        print(\"✅ Confident policy target works fine\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with confident policy: {e}\")\n",
    "\n",
    "# Test with moderate policy\n",
    "try:\n",
    "    dummy_targets_moderate = {'policy': moderate_policy, 'value': torch.tensor([1.0], device=device)}\n",
    "    \n",
    "    loss_moderate, metrics_moderate = training_adapter.compute_loss(\n",
    "        model_output, dummy_targets_moderate, dummy_extra_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Loss with moderate policy target: {loss_moderate.item()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with moderate policy: {e}\")\n",
    "\n",
    "# Test what happens when model outputs extreme logits\n",
    "print(f\"\\n--- Testing Extreme Model Outputs ---\")\n",
    "try:\n",
    "    # Very confident model output\n",
    "    extreme_model_output = {\n",
    "        'policy': torch.tensor([[10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0]], device=device),\n",
    "        'value': torch.tensor([0.5], device=device)\n",
    "    }\n",
    "    \n",
    "    loss_extreme, metrics_extreme = training_adapter.compute_loss(\n",
    "        extreme_model_output, dummy_targets_moderate, dummy_extra_data\n",
    "    )\n",
    "    \n",
    "    print(f\"Loss with extreme model output: {loss_extreme.item()}\")\n",
    "    if torch.isnan(loss_extreme):\n",
    "        print(\"❌ Extreme model output causes NaN loss!\")\n",
    "    else:\n",
    "        print(\"✅ Extreme model output handled correctly\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with extreme model output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_level: str = \"INFO\"):\n",
    "    \"\"\"Setup logging configuration.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE TRAINING SCRIPT SIMULATION ===\n",
      "Using device: mps\n",
      "Model parameters: {'input_dim': 84, 'num_residual_blocks': 10, 'residual_dim': 64, 'hidden_size': 256, 'policy_head_dim': 7}\n",
      "Training parameters:\n",
      "  Batch size: 32\n",
      "  Buffer max size: 320\n",
      "  Buffer min size: 32\n",
      "  Min new examples per step: 128\n",
      "\n",
      "=== STEP 1: Generate Initial Training Data ===\n",
      "Generating 1 games to reach minimum buffer size...\n",
      "Generated batch 1, total examples: 66\n",
      "Generated 66 total examples\n",
      "\n",
      "=== STEP 2: Check Examples Before Encoding ===\n",
      "Found 0 examples with NaN out of 66\n",
      "\n",
      "=== STEP 3: Encode and Fill Buffer ===\n",
      "Added batch 1, buffer size: 66\n",
      "✅ Buffer filled to 66 examples\n",
      "\n",
      "=== STEP 4: Simulate Training Loop ===\n",
      "Starting training simulation...\n",
      "\n",
      "--- Training Step 1 ---\n",
      "Step 1: Loss = 0.823926\n",
      "  policy_loss: 0.482449\n",
      "  value_loss: 0.341477\n",
      "\n",
      "--- Training Step 2 ---\n",
      "Step 2: Loss = 1.175985\n",
      "  policy_loss: 0.440660\n",
      "  value_loss: 0.735325\n",
      "\n",
      "--- Training Step 3 ---\n",
      "Step 3: Loss = 1.230045\n",
      "  policy_loss: 0.576644\n",
      "  value_loss: 0.653401\n",
      "\n",
      "--- Training Step 4 ---\n",
      "Step 4: Loss = 0.938519\n",
      "  policy_loss: 0.423158\n",
      "  value_loss: 0.515361\n",
      "\n",
      "--- Training Step 5 ---\n",
      "Step 5: Loss = 0.679497\n",
      "  policy_loss: 0.416376\n",
      "  value_loss: 0.263121\n",
      "\n",
      "--- Training Step 6 ---\n",
      "Step 6: Loss = 0.758740\n",
      "  policy_loss: 0.402682\n",
      "  value_loss: 0.356058\n",
      "Simulating new examples from actors...\n",
      "Added 55 new examples, buffer size: 121\n",
      "\n",
      "--- Training Step 7 ---\n",
      "Step 7: Loss = 0.989504\n",
      "  policy_loss: 0.685724\n",
      "  value_loss: 0.303779\n",
      "\n",
      "--- Training Step 8 ---\n",
      "Step 8: Loss = 0.830974\n",
      "  policy_loss: 0.545449\n",
      "  value_loss: 0.285525\n",
      "\n",
      "--- Training Step 9 ---\n",
      "Step 9: Loss = 0.856926\n",
      "  policy_loss: 0.570445\n",
      "  value_loss: 0.286481\n",
      "\n",
      "--- Training Step 10 ---\n",
      "Step 10: Loss = 0.869419\n",
      "  policy_loss: 0.466058\n",
      "  value_loss: 0.403362\n",
      "\n",
      "--- Training Step 11 ---\n",
      "Step 11: Loss = 0.799445\n",
      "  policy_loss: 0.574339\n",
      "  value_loss: 0.225106\n",
      "Simulating new examples from actors...\n",
      "Added 58 new examples, buffer size: 179\n",
      "\n",
      "--- Training Step 12 ---\n",
      "Step 12: Loss = 0.881666\n",
      "  policy_loss: 0.449990\n",
      "  value_loss: 0.431675\n",
      "\n",
      "--- Training Step 13 ---\n",
      "Step 13: Loss = 1.005482\n",
      "  policy_loss: 0.462809\n",
      "  value_loss: 0.542673\n",
      "\n",
      "--- Training Step 14 ---\n",
      "Step 14: Loss = 0.848847\n",
      "  policy_loss: 0.336056\n",
      "  value_loss: 0.512791\n",
      "\n",
      "--- Training Step 15 ---\n",
      "Step 15: Loss = 0.916108\n",
      "  policy_loss: 0.431485\n",
      "  value_loss: 0.484624\n",
      "\n",
      "--- Training Step 16 ---\n",
      "Step 16: Loss = 1.204661\n",
      "  policy_loss: 0.558508\n",
      "  value_loss: 0.646153\n",
      "Simulating new examples from actors...\n",
      "Added 47 new examples, buffer size: 226\n",
      "\n",
      "--- Training Step 17 ---\n",
      "Step 17: Loss = 1.189486\n",
      "  policy_loss: 0.591074\n",
      "  value_loss: 0.598411\n",
      "\n",
      "--- Training Step 18 ---\n",
      "Step 18: Loss = 0.939743\n",
      "  policy_loss: 0.387796\n",
      "  value_loss: 0.551947\n",
      "\n",
      "--- Training Step 19 ---\n",
      "Step 19: Loss = 0.925407\n",
      "  policy_loss: 0.399741\n",
      "  value_loss: 0.525667\n",
      "\n",
      "--- Training Step 20 ---\n",
      "Step 20: Loss = 1.251069\n",
      "  policy_loss: 0.521383\n",
      "  value_loss: 0.729686\n",
      "\n",
      "=== STEP 5: Final Buffer Analysis ===\n",
      "Analyzing final buffer state...\n",
      "Buffer region around index 0: 0 NaN values, 0 NaN policies\n",
      "Buffer region around index 56: 0 NaN values, 0 NaN policies\n",
      "Buffer region around index 113: 0 NaN values, 0 NaN policies\n",
      "Buffer region around index 169: 0 NaN values, 0 NaN policies\n",
      "Buffer region around index 225: 0 NaN values, 0 NaN policies\n",
      "\n",
      "=== Training Summary ===\n",
      "Completed 20 training steps\n",
      "Loss progression: [0.8239256143569946, 1.1759852170944214, 1.2300450801849365, 0.9385187029838562, 0.6794966459274292] ... [1.2046613693237305, 1.189485788345337, 0.9397425651550293, 0.9254074692726135, 1.2510688304901123]\n",
      "Final buffer size: 226\n",
      "Loss range: 0.679497 to 1.251069\n",
      "\n",
      "=== Device-Specific Checks ===\n",
      "Running on MPS - checking for known MPS issues...\n",
      "✅ MPS autocast working correctly\n",
      "\n",
      "=== CONCLUSION ===\n",
      "Check the output above for:\n",
      "1. ❌ NaN in trajectory rewards (data generation issue)\n",
      "2. ❌ NaN in model outputs (model instability)\n",
      "3. ❌ NaN in loss computation (numerical instability)\n",
      "4. ❌ NaN in model parameters (training instability)\n",
      "5. ❌ Loss explosion or MPS-specific issues\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyspiel\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import itertools\n",
    "\n",
    "# Import your modules\n",
    "from core.games.open_spiel_state_wrapper import OpenSpielState\n",
    "from core.algorithms.AlphaZero import AlphaZero, AlphaZeroConfig, AlphaZeroTrainingAdapter\n",
    "from core.model_interface import Model, ModelPredictor\n",
    "from core.simulation import generate_trajectories\n",
    "from core.data_structures import ReplayBuffer\n",
    "from experiments.connect_four.tensor_mapping import ConnectFourTensorMapping\n",
    "from experiments.connect_four.models.resmlp import ResMLP, ResMLPInitParams\n",
    "\n",
    "print(\"=== COMPREHENSIVE TRAINING SCRIPT SIMULATION ===\")\n",
    "\n",
    "# Setup - exactly matching train.py\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def state_factory():\n",
    "    game = pyspiel.load_game(\"connect_four\")\n",
    "    return OpenSpielState(game.new_initial_state(), num_players=2)\n",
    "\n",
    "# EXACT model parameters from train.py\n",
    "model_params = ResMLPInitParams(\n",
    "    input_dim=2 * 6 * 7, \n",
    "    num_residual_blocks=10, \n",
    "    residual_dim=64, \n",
    "    hidden_size=256, \n",
    "    policy_head_dim=7\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model_params}\")\n",
    "\n",
    "# Initialize model exactly as in train.py\n",
    "model = Model(\n",
    "    model_architecture=ResMLP,\n",
    "    init_params=model_params,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# EXACT training components from train.py\n",
    "tensor_mapping = ConnectFourTensorMapping()\n",
    "training_adapter = AlphaZeroTrainingAdapter()\n",
    "alphazero_config = AlphaZeroConfig()  # Default config as in train.py\n",
    "model_predictor = ModelPredictor(model, tensor_mapping)\n",
    "\n",
    "# EXACT optimizer from train.py\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.model.parameters(),\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-4,\n",
    "    amsgrad=False\n",
    ")\n",
    "\n",
    "# EXACT learning rate scheduler from train.py\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    factor=0.9,\n",
    "    patience=1_000,\n",
    "    cooldown=1_000,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "# EXACT buffer parameters from train.py – NOTE: changed from 256 to 32\n",
    "learning_batch_size = 32\n",
    "buffer_max_size = 10 * 32  # 25,600\n",
    "learning_min_buffer_size = 32  # 7,680\n",
    "learning_min_new_examples_per_step = 128\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_max_size, device=device)\n",
    "\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Batch size: {learning_batch_size}\")\n",
    "print(f\"  Buffer max size: {buffer_max_size}\")\n",
    "print(f\"  Buffer min size: {learning_min_buffer_size}\")\n",
    "print(f\"  Min new examples per step: {learning_min_new_examples_per_step}\")\n",
    "\n",
    "print(\"\\n=== STEP 1: Generate Initial Training Data ===\")\n",
    "# Generate enough data to reach minimum buffer size\n",
    "total_examples_needed = learning_min_buffer_size\n",
    "games_to_generate = total_examples_needed // 20  # Estimate ~20 examples per game\n",
    "\n",
    "print(f\"Generating {games_to_generate} games to reach minimum buffer size...\")\n",
    "\n",
    "all_examples = []\n",
    "for game_batch in range(games_to_generate):  # Process in batches to avoid memory issues\n",
    "    try:\n",
    "        trajectories = generate_trajectories(\n",
    "            initial_state_creator=state_factory,\n",
    "            trajectory_player_creators=[\n",
    "                lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config),\n",
    "                lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config)\n",
    "            ],\n",
    "            opponent_creators=[],\n",
    "            num_games=2,\n",
    "            logger=logger\n",
    "        )\n",
    "        \n",
    "        # Extract examples\n",
    "        batch_examples = []\n",
    "        for trajectory in trajectories:\n",
    "            batch_examples.extend(training_adapter.extract_examples(trajectory))\n",
    "        \n",
    "        all_examples.extend(batch_examples)\n",
    "        print(f\"Generated batch {game_batch + 1}, total examples: {len(all_examples)}\")\n",
    "        \n",
    "        # Check for NaN in trajectory rewards\n",
    "        for i, trajectory in enumerate(trajectories):\n",
    "            if np.isnan(trajectory[-1].reward):\n",
    "                print(f\"❌ NaN in trajectory {game_batch + i} final reward!\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating game batch {game_batch//10 + 1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"Generated {len(all_examples)} total examples\")\n",
    "\n",
    "# Check examples for NaN before encoding\n",
    "print(\"\\n=== STEP 2: Check Examples Before Encoding ===\")\n",
    "nan_examples = 0\n",
    "for i, example in enumerate(all_examples):\n",
    "    policy_target, value_target = example.target\n",
    "    if np.isnan(value_target) or any(np.isnan(v) for v in policy_target.values()):\n",
    "        print(f\"❌ NaN in example {i}: value={value_target}, policy_has_nan={any(np.isnan(v) for v in policy_target.values())}\")\n",
    "        nan_examples += 1\n",
    "        \n",
    "print(f\"Found {nan_examples} examples with NaN out of {len(all_examples)}\")\n",
    "\n",
    "print(\"\\n=== STEP 3: Encode and Fill Buffer ===\")\n",
    "try:\n",
    "    # Encode examples in batches to match training script behavior\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(all_examples), batch_size):\n",
    "        batch_examples = all_examples[i:i+batch_size]\n",
    "        states, targets, extra_data = tensor_mapping.encode_examples(batch_examples, device)\n",
    "        replay_buffer.add(states, targets, extra_data)\n",
    "        \n",
    "        # Check for NaN in encoded data\n",
    "        if torch.isnan(targets['value']).any():\n",
    "            nan_indices = torch.isnan(targets['value']).nonzero().flatten()\n",
    "            print(f\"❌ NaN values found in batch {i//batch_size + 1} at indices: {nan_indices[:5]}...\")\n",
    "        \n",
    "        print(f\"Added batch {i//batch_size + 1}, buffer size: {len(replay_buffer)}\")\n",
    "        \n",
    "        if len(replay_buffer) >= learning_min_buffer_size:\n",
    "            break\n",
    "            \n",
    "    print(f\"✅ Buffer filled to {len(replay_buffer)} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error filling buffer: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== STEP 4: Simulate Training Loop ===\")\n",
    "# Simulate the exact training loop from training.py\n",
    "training_steps = 20  # Enough to see if NaN develops\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training simulation...\")\n",
    "for step in range(training_steps):\n",
    "    try:\n",
    "        print(f\"\\n--- Training Step {step + 1} ---\")\n",
    "        \n",
    "        # Sample batch exactly as in training script\n",
    "        states, targets, extra_data = replay_buffer.sample(learning_batch_size)\n",
    "        \n",
    "        # Check sampled data for NaN\n",
    "        if torch.isnan(targets['value']).any():\n",
    "            nan_count = torch.isnan(targets['value']).sum().item()\n",
    "            print(f\"❌ Sampled batch contains {nan_count} NaN values!\")\n",
    "            \n",
    "        if torch.isnan(targets['policy']).any():\n",
    "            nan_count = torch.isnan(targets['policy']).sum().item()\n",
    "            print(f\"❌ Sampled batch contains {nan_count} NaN policies!\")\n",
    "        \n",
    "        # EXACT training step from training.py\n",
    "        model.model.train()\n",
    "        \n",
    "        # Use autocast exactly as in training script\n",
    "        with torch.autocast(device_type=device.type):\n",
    "            model_outputs = model.model(states)\n",
    "            \n",
    "            # Check model outputs\n",
    "            if torch.isnan(model_outputs['policy']).any():\n",
    "                print(f\"❌ Model output policy contains NaN!\")\n",
    "                print(f\"Policy stats: min={model_outputs['policy'].min()}, max={model_outputs['policy'].max()}\")\n",
    "                \n",
    "            if torch.isnan(model_outputs['value']).any():\n",
    "                print(f\"❌ Model output value contains NaN!\")\n",
    "                print(f\"Value stats: min={model_outputs['value'].min()}, max={model_outputs['value'].max()}\")\n",
    "            \n",
    "            loss, metrics = training_adapter.compute_loss(model_outputs, targets, extra_data)\n",
    "        \n",
    "        # Check loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"❌ Loss is NaN at step {step + 1}!\")\n",
    "            print(\"Debugging loss components...\")\n",
    "            \n",
    "            # Manual loss debugging\n",
    "            policy_logits = model_outputs[\"policy\"]\n",
    "            value_preds = model_outputs[\"value\"]\n",
    "            policy_targets = targets['policy']\n",
    "            value_targets = targets['value']\n",
    "            legal_actions_mask = extra_data['legal_actions']\n",
    "            \n",
    "            print(f\"  Policy logits: min={policy_logits.min()}, max={policy_logits.max()}, nan_count={torch.isnan(policy_logits).sum()}\")\n",
    "            print(f\"  Value preds: min={value_preds.min()}, max={value_preds.max()}, nan_count={torch.isnan(value_preds).sum()}\")\n",
    "            print(f\"  Policy targets: min={policy_targets.min()}, max={policy_targets.max()}, nan_count={torch.isnan(policy_targets).sum()}\")\n",
    "            print(f\"  Value targets: min={value_targets.min()}, max={value_targets.max()}, nan_count={torch.isnan(value_targets).sum()}\")\n",
    "            \n",
    "            break\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Check model parameters for NaN after update\n",
    "        nan_params = []\n",
    "        for name, param in model.model.named_parameters():\n",
    "            if torch.isnan(param).any():\n",
    "                nan_params.append(name)\n",
    "                \n",
    "        if nan_params:\n",
    "            print(f\"❌ NaN in model parameters: {nan_params}\")\n",
    "            break\n",
    "        \n",
    "        # Update learning rate (every step to match training script)\n",
    "        if step > 0:\n",
    "            lr_scheduler.step(loss.item())\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "        print(f\"Step {step + 1}: Loss = {loss.item():.6f}\")\n",
    "        if metrics:\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                print(f\"  {metric_name}: {metric_value:.6f}\")\n",
    "        \n",
    "        # Simulate adding new examples (as would happen in multi-actor setup)\n",
    "        if step % 5 == 0 and step > 0:  # Every 5 steps, add some new examples\n",
    "            print(\"Simulating new examples from actors...\")\n",
    "            try:\n",
    "                new_trajectories = generate_trajectories(\n",
    "                    initial_state_creator=state_factory,\n",
    "                    trajectory_player_creators=[\n",
    "                        lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config),\n",
    "                        lambda state: training_adapter.create_tree_search(state.clone(), model_predictor, alphazero_config)\n",
    "                    ],\n",
    "                    opponent_creators=[],\n",
    "                    num_games=2,\n",
    "                    logger=logger\n",
    "                )\n",
    "                \n",
    "                new_examples = []\n",
    "                for trajectory in new_trajectories:\n",
    "                    new_examples.extend(training_adapter.extract_examples(trajectory))\n",
    "                \n",
    "                # Check new examples for NaN\n",
    "                new_nan_count = 0\n",
    "                for example in new_examples:\n",
    "                    _, value_target = example.target\n",
    "                    if np.isnan(value_target):\n",
    "                        new_nan_count += 1\n",
    "                \n",
    "                if new_nan_count > 0:\n",
    "                    print(f\"❌ {new_nan_count} new examples contain NaN values!\")\n",
    "                \n",
    "                # Add to buffer\n",
    "                states_new, targets_new, extra_data_new = tensor_mapping.encode_examples(new_examples, device)\n",
    "                replay_buffer.add(states_new, targets_new, extra_data_new)\n",
    "                \n",
    "                print(f\"Added {len(new_examples)} new examples, buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "                all_examples.extend(new_examples)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error generating new examples: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in training step {step + 1}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "\n",
    "print(f\"\\n=== STEP 5: Final Buffer Analysis ===\")\n",
    "# Check final buffer state\n",
    "print(\"Analyzing final buffer state...\")\n",
    "\n",
    "# Sample from different parts of the buffer to check for corruption\n",
    "buffer_check_indices = [0, len(replay_buffer)//4, len(replay_buffer)//2, 3*len(replay_buffer)//4, len(replay_buffer)-1]\n",
    "\n",
    "for idx in buffer_check_indices:\n",
    "    try:\n",
    "        # Sample around this index\n",
    "        sample_states, sample_targets, sample_extra_data = replay_buffer.sample(min(10, len(replay_buffer)))\n",
    "        \n",
    "        nan_values = torch.isnan(sample_targets['value']).sum().item()\n",
    "        nan_policies = torch.isnan(sample_targets['policy']).sum().item()\n",
    "        \n",
    "        print(f\"Buffer region around index {idx}: {nan_values} NaN values, {nan_policies} NaN policies\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking buffer region {idx}: {e}\")\n",
    "\n",
    "print(f\"\\n=== Training Summary ===\")\n",
    "print(f\"Completed {len(losses)} training steps\")\n",
    "print(f\"Loss progression: {losses[:5]} ... {losses[-5:] if len(losses) > 5 else losses}\")\n",
    "print(f\"Final buffer size: {len(replay_buffer)}\")\n",
    "\n",
    "if losses:\n",
    "    print(f\"Loss range: {min(losses):.6f} to {max(losses):.6f}\")\n",
    "    \n",
    "    # Check for loss explosion\n",
    "    if any(loss > 100 for loss in losses):\n",
    "        print(\"❌ Loss explosion detected!\")\n",
    "    \n",
    "    if any(np.isnan(loss) for loss in losses):\n",
    "        print(\"❌ NaN losses detected!\")\n",
    "else:\n",
    "    print(\"❌ No successful training steps completed!\")\n",
    "\n",
    "print(\"\\n=== Device-Specific Checks ===\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Running on MPS - checking for known MPS issues...\")\n",
    "    \n",
    "    # Test autocast specifically on MPS\n",
    "    try:\n",
    "        test_tensor = torch.randn(10, 84, device=device)\n",
    "        with torch.autocast(device_type='mps'):\n",
    "            output = model.model(test_tensor)\n",
    "            if torch.isnan(output['policy']).any() or torch.isnan(output['value']).any():\n",
    "                print(\"❌ MPS autocast producing NaN outputs!\")\n",
    "            else:\n",
    "                print(\"✅ MPS autocast working correctly\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ MPS autocast error: {e}\")\n",
    "\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "print(\"Check the output above for:\")\n",
    "print(\"1. ❌ NaN in trajectory rewards (data generation issue)\")\n",
    "print(\"2. ❌ NaN in model outputs (model instability)\")\n",
    "print(\"3. ❌ NaN in loss computation (numerical instability)\")\n",
    "print(\"4. ❌ NaN in model parameters (training instability)\")\n",
    "print(\"5. ❌ Loss explosion or MPS-specific issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED INSPECTION OF CORRUPTED EXAMPLES ===\n",
      "Total examples: 226\n",
      "Valid examples: 226\n",
      "Corrupted examples: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== DETAILED INSPECTION OF CORRUPTED EXAMPLES ===\")\n",
    "\n",
    "def inspect_corrupted_examples(all_examples):\n",
    "    \"\"\"Inspect examples and show detailed data for corrupted ones.\"\"\"\n",
    "    \n",
    "    corrupted_examples = []\n",
    "    valid_examples = []\n",
    "    \n",
    "    for i, example in enumerate(all_examples):\n",
    "        policy_dict, value = example.target\n",
    "        \n",
    "        # Check for NaN in value\n",
    "        value_is_nan = np.isnan(value) if isinstance(value, (int, float)) else torch.isnan(value).any()\n",
    "        \n",
    "        # Check for NaN in policy\n",
    "        policy_has_nan = False\n",
    "        if isinstance(policy_dict, dict):\n",
    "            policy_has_nan = any(np.isnan(v) if isinstance(v, (int, float)) else torch.isnan(v).any() \n",
    "                               for v in policy_dict.values())\n",
    "        \n",
    "        if value_is_nan or policy_has_nan:\n",
    "            corrupted_examples.append((i, example))\n",
    "        else:\n",
    "            valid_examples.append((i, example))\n",
    "    \n",
    "    print(f\"Total examples: {len(all_examples)}\")\n",
    "    print(f\"Valid examples: {len(valid_examples)}\")\n",
    "    print(f\"Corrupted examples: {len(corrupted_examples)}\")\n",
    "    \n",
    "    if corrupted_examples:\n",
    "        print(f\"\\n=== DETAILED ANALYSIS OF CORRUPTED EXAMPLES ===\")\n",
    "        \n",
    "        for idx, (example_idx, example) in enumerate(corrupted_examples[:5]):  # Show first 5 corrupted\n",
    "            print(f\"\\n--- Corrupted Example #{idx+1} (Index {example_idx}) ---\")\n",
    "            \n",
    "            # Extract target components\n",
    "            policy_dict, value = example.target\n",
    "            \n",
    "            print(f\"State type: {type(example.state)}\")\n",
    "            print(f\"State:\\n{example.state}\")\n",
    "            \n",
    "            # Check state properties if it's an OpenSpielState\n",
    "            if hasattr(example.state, 'current_player'):\n",
    "                print(f\"Current player: {example.state.current_player}\")\n",
    "            if hasattr(example.state, 'is_terminal'):\n",
    "                print(f\"Is terminal: {example.state.is_terminal}\")\n",
    "            if hasattr(example.state, 'rewards'):\n",
    "                print(f\"State rewards: {example.state.rewards}\")\n",
    "            if hasattr(example.state, 'legal_actions'):\n",
    "                print(f\"Legal actions: {example.state.legal_actions}\")\n",
    "            \n",
    "            # Analyze target\n",
    "            print(f\"\\nTarget Analysis:\")\n",
    "            print(f\"Policy type: {type(policy_dict)}\")\n",
    "            print(f\"Policy: {policy_dict}\")\n",
    "            \n",
    "            if isinstance(policy_dict, dict):\n",
    "                policy_sum = sum(policy_dict.values())\n",
    "                print(f\"Policy sum: {policy_sum}\")\n",
    "                nan_actions = [action for action, prob in policy_dict.items() if np.isnan(prob)]\n",
    "                if nan_actions:\n",
    "                    print(f\"❌ Actions with NaN probabilities: {nan_actions}\")\n",
    "                    \n",
    "            print(f\"Value type: {type(value)}\")\n",
    "            print(f\"Value: {value}\")\n",
    "            if np.isnan(value):\n",
    "                print(f\"❌ Value is NaN!\")\n",
    "                \n",
    "            # Analyze extra_data\n",
    "            print(f\"\\nExtra Data:\")\n",
    "            for key, val in example.extra_data.items():\n",
    "                print(f\"  {key}: {val}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    # Show some valid examples for comparison\n",
    "    if valid_examples and corrupted_examples:\n",
    "        print(f\"\\n=== COMPARISON: VALID EXAMPLES ===\")\n",
    "        \n",
    "        for idx, (example_idx, example) in enumerate(valid_examples[:2]):  # Show first 2 valid\n",
    "            print(f\"\\n--- Valid Example #{idx+1} (Index {example_idx}) ---\")\n",
    "            \n",
    "            policy_dict, value = example.target\n",
    "            \n",
    "            print(f\"State:\\n{example.state}\")\n",
    "            if hasattr(example.state, 'current_player'):\n",
    "                print(f\"Current player: {example.state.current_player}\")\n",
    "            if hasattr(example.state, 'is_terminal'):\n",
    "                print(f\"Is terminal: {example.state.is_terminal}\")\n",
    "            if hasattr(example.state, 'rewards'):\n",
    "                print(f\"State rewards: {example.state.rewards}\")\n",
    "                \n",
    "            print(f\"Policy: {policy_dict}\")\n",
    "            print(f\"Policy sum: {sum(policy_dict.values()) if isinstance(policy_dict, dict) else 'N/A'}\")\n",
    "            print(f\"Value: {value}\")\n",
    "            print(\"-\" * 40)\n",
    "    \n",
    "    return corrupted_examples, valid_examples\n",
    "\n",
    "# Inspect your examples\n",
    "corrupted, valid = inspect_corrupted_examples(all_examples)\n",
    "\n",
    "# Additional analysis\n",
    "if corrupted:\n",
    "    print(f\"\\n=== CORRUPTION PATTERNS ===\")\n",
    "    \n",
    "    # Check if corruption happens at specific indices\n",
    "    corrupted_indices = [idx for idx, _ in corrupted]\n",
    "    print(f\"Corrupted example indices: {corrupted_indices[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Check if corruption correlates with specific states\n",
    "    terminal_states = 0\n",
    "    non_terminal_states = 0\n",
    "    unknown_states = 0\n",
    "    \n",
    "    for _, example in corrupted:\n",
    "        if hasattr(example.state, 'is_terminal'):\n",
    "            if example.state.is_terminal:\n",
    "                terminal_states += 1\n",
    "            else:\n",
    "                non_terminal_states += 1\n",
    "        else:\n",
    "            unknown_states += 1\n",
    "    \n",
    "    print(f\"Corrupted examples by state type:\")\n",
    "    print(f\"  Terminal states: {terminal_states}\")\n",
    "    print(f\"  Non-terminal states: {non_terminal_states}\")\n",
    "    print(f\"  Unknown state type: {unknown_states}\")\n",
    "    \n",
    "    # Check corruption types\n",
    "    value_nan_count = 0\n",
    "    policy_nan_count = 0\n",
    "    both_nan_count = 0\n",
    "    \n",
    "    for _, example in corrupted:\n",
    "        policy_dict, value = example.target\n",
    "        \n",
    "        value_is_nan = np.isnan(value)\n",
    "        policy_has_nan = any(np.isnan(v) for v in policy_dict.values()) if isinstance(policy_dict, dict) else False\n",
    "        \n",
    "        if value_is_nan and policy_has_nan:\n",
    "            both_nan_count += 1\n",
    "        elif value_is_nan:\n",
    "            value_nan_count += 1\n",
    "        elif policy_has_nan:\n",
    "            policy_nan_count += 1\n",
    "    \n",
    "    print(f\"Corruption breakdown:\")\n",
    "    print(f\"  Value NaN only: {value_nan_count}\")\n",
    "    print(f\"  Policy NaN only: {policy_nan_count}\")\n",
    "    print(f\"  Both NaN: {both_nan_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
