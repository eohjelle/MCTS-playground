{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/eohjelle/Documents/2025-mcts-playground/mcts-playground\n"
     ]
    }
   ],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('../../..')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will create an exhaustive training data set for tic tac toe using the Minimax agent, in the form of a replay buffer compatible with AlphaZeroTrainer. The idea is to use this dataset to run some sweeps, and to understand which deep learning models will perform best in theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m state = OpenSpielState(game.new_initial_state(), num_players=\u001b[32m2\u001b[39m)\n\u001b[32m      8\u001b[39m agent = Minimax(state)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# # Get list of unique examples, translated into AlphaZero format for compatibility with models\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# def example(state, node):\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     policy = {action: 0.0 for action in node.children.keys()}\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#     for action in node.value.best_actions:\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#         policy[action] = 1/len(node.value.best_actions)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/algorithms/Minimax.py:48\u001b[39m, in \u001b[36mMinimax.__call__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ActionType:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.root.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mRoot value is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.root.value.best_actions) > \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBest actions are empty\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/algorithms/Minimax.py:37\u001b[39m, in \u001b[36mMinimax.evaluate\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     sign = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.state.current_player == child.state.current_player \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;66;03m# Some games have repeated moves\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     child_value = sign * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m.value \u001b[38;5;66;03m# Two player zero sum game\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m child_value > max_value:\n\u001b[32m     39\u001b[39m     max_value = child_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/algorithms/Minimax.py:37\u001b[39m, in \u001b[36mMinimax.evaluate\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     sign = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.state.current_player == child.state.current_player \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;66;03m# Some games have repeated moves\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     child_value = sign * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m.value \u001b[38;5;66;03m# Two player zero sum game\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m child_value > max_value:\n\u001b[32m     39\u001b[39m     max_value = child_value\n",
      "    \u001b[31m[... skipping similar frames: Minimax.evaluate at line 37 (32 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/algorithms/Minimax.py:37\u001b[39m, in \u001b[36mMinimax.evaluate\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     sign = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.state.current_player == child.state.current_player \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;66;03m# Some games have repeated moves\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     child_value = sign * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m.value \u001b[38;5;66;03m# Two player zero sum game\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m child_value > max_value:\n\u001b[32m     39\u001b[39m     max_value = child_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/algorithms/Minimax.py:36\u001b[39m, in \u001b[36mMinimax.evaluate\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m     34\u001b[39m     child_value = child.state.rewards[node.state.current_player]\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     sign = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node.state.current_player == \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_player\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m \u001b[38;5;66;03m# Some games have repeated moves\u001b[39;00m\n\u001b[32m     37\u001b[39m     child_value = sign * \u001b[38;5;28mself\u001b[39m.evaluate(child).value \u001b[38;5;66;03m# Two player zero sum game\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m child_value > max_value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/2025-mcts-playground/mcts-playground/core/games/open_spiel_state_wrapper.py:39\u001b[39m, in \u001b[36mOpenSpielState.current_player\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcurrent_player\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspiel_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_player\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from core.games.open_spiel_state_wrapper import OpenSpielState\n",
    "from core.algorithms import Minimax\n",
    "import pyspiel\n",
    "\n",
    "# Creat minmax agent and expand the game tree\n",
    "game = pyspiel.load_game(\"connect_four\")\n",
    "state = OpenSpielState(game.new_initial_state(), num_players=2)\n",
    "agent = Minimax(state)\n",
    "agent()\n",
    "\n",
    "# # Get list of unique examples, translated into AlphaZero format for compatibility with models\n",
    "\n",
    "# def example(state, node):\n",
    "#     policy = {action: 0.0 for action in node.children.keys()}\n",
    "#     for action in node.value.best_actions:\n",
    "#         policy[action] = 1/len(node.value.best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 8\n",
      "Number of states: 8\n"
     ]
    }
   ],
   "source": [
    "# This is for testing the state_dict design\n",
    "\n",
    "def count_nodes(root):\n",
    "    if root.is_leaf():\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + sum(count_nodes(child) for child in root.children.values())\n",
    "\n",
    "print(f\"Number of nodes: {count_nodes(agent.root)}\")\n",
    "\n",
    "def count_states(agent):\n",
    "    return len(agent.state_dict)\n",
    "\n",
    "print(f\"Number of states: {count_states(agent)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "\n",
      "MinimaxValue(player=0, value=0.0, best_actions=[0, 1, 2, 3, 4, 5, 6])\n",
      "{0: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x13457ccd0>, value=None, children={}), 1: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x13457cf50>, value=None, children={}), 2: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x134530fc0>, value=None, children={}), 3: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x134531220>, value=None, children={}), 4: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x1345a27b0>, value=None, children={}), 5: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x1344c3460>, value=None, children={}), 6: Node(state=<core.games.open_spiel_state_wrapper.OpenSpielState object at 0x1344c3680>, value=None, children={})}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "x......\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".x.....\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "..x....\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...x...\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "....x..\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".....x.\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "......x\n",
      "\n",
      "None\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for state, node in agent.state_dict.items():\n",
    "    print(state)\n",
    "    print(node.value)\n",
    "    print(node.children)\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique examples, translated into AlphaZero format for compatibility with models\n",
    "\n",
    "from core.data_structures import TrainingExample\n",
    "\n",
    "def example(state, node):\n",
    "    policy = {action: 0.0 for action in node.children.keys()}\n",
    "    for action in node.value.best_actions:\n",
    "        policy[action] = 1/len(node.value.best_actions)\n",
    "    return TrainingExample(\n",
    "        state=state,\n",
    "        target=(policy, node.value.value),\n",
    "        extra_data={'legal_actions': list(node.children.keys())}\n",
    "    )\n",
    "\n",
    "examples = [example(state, node) for state, node in agent.state_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | \n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 2): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 2)]}\n",
      "\n",
      "\n",
      "Example 2:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      " | X | O\n",
      "Target: ({(0, 0): 1.0, (1, 2): 0.0, (2, 0): 0.0}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2), (2, 0)]}\n",
      "\n",
      "\n",
      "Example 3:\n",
      "State: \n",
      " | X | X\n",
      "---------\n",
      "O | O | \n",
      "---------\n",
      "O | X | X\n",
      "Target: ({(0, 0): 0.5, (1, 2): 0.5}, 1.0)\n",
      "Data: {'legal_actions': [(0, 0), (1, 2)]}\n",
      "\n",
      "\n",
      "Number of unique examples: 5478\n"
     ]
    }
   ],
   "source": [
    "k = 2836\n",
    "for i, example in enumerate(examples[k:k+3]):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"State: \\n{example.state}\")\n",
    "    print(f\"Target: {example.target}\")\n",
    "    print(f\"Data: {example.extra_data}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Number of unique examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 0])\n",
      "Tokenized policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True, False, False,  True])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
      "MLP policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False, False, False,  True,  True,  True,  True,  True])\n",
      "\n",
      "Example 2:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 0, 1, 2])\n",
      "Tokenized policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True,  True, False, False])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "MLP policy: tensor([1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False, False,  True,  True,  True,  True,  True, False])\n",
      "\n",
      "Example 3:\n",
      "Tokenized state: tensor([0, 1, 1, 2, 2, 0, 2, 1, 1])\n",
      "Tokenized policy: tensor([0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000])\n",
      "Tokenized value: 1.0\n",
      "Tokenized legal actions: tensor([ True, False, False, False, False,  True, False, False, False])\n",
      "MLP state: tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.])\n",
      "MLP policy: tensor([0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 0.0000, 0.0000])\n",
      "MLP value: 1.0\n",
      "MLP legal actions: tensor([ True, False,  True,  True,  True,  True,  True, False, False])\n"
     ]
    }
   ],
   "source": [
    "# Test encoding and decoding\n",
    "\n",
    "from experiments.experimenting_with_model_architectures_in_tic_tac_toe.tensor_mapping import MLPTensorMapping, TokenizedTensorMapping\n",
    "import torch\n",
    "\n",
    "tokenized_states, tokenized_targets, tokenized_data = TokenizedTensorMapping.encode_examples(examples, device=torch.device('cpu'))\n",
    "mlp_states, mlp_targets, mlp_data = MLPTensorMapping.encode_examples(examples, device=torch.device('cpu'))\n",
    "\n",
    "for i in range(k, k+3):\n",
    "    print(f\"\\nExample {i+1-k}:\")\n",
    "    print(f\"Tokenized state: {tokenized_states[i]}\")\n",
    "    print(f\"Tokenized policy: {tokenized_targets['policy'][i]}\")\n",
    "    print(f\"Tokenized value: {tokenized_targets['value'][i]}\")\n",
    "    print(f\"Tokenized legal actions: {tokenized_data['legal_actions'][i]}\")\n",
    "    print(f\"MLP state: {mlp_states[i]}\")\n",
    "    print(f\"MLP policy: {mlp_targets['policy'][i]}\")\n",
    "    print(f\"MLP value: {mlp_targets['value'][i]}\")\n",
    "    print(f\"MLP legal actions: {mlp_data['legal_actions'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sample entropies: tensor([2.1972, -0.0000, 1.3863, -0.0000, 1.3863, 1.3863, 1.3863, -0.0000, 1.3863,\n",
      "        -0.0000])\n",
      "Average entropy: 0.40638184547424316\n",
      "KL divergence between target and random logits: 1.742671251296997\n",
      "KL divergence between target and uniform logits: 1.406589150428772\n"
     ]
    }
   ],
   "source": [
    "# Check entropy of target distribution\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "targets = tokenized_targets\n",
    "\n",
    "non_zero_policy = torch.where(targets['policy'] > 0, targets['policy'], torch.ones_like(targets['policy']))\n",
    "logits = torch.log(non_zero_policy)\n",
    "entropy = - torch.sum(targets['policy'] * logits, dim=1)\n",
    "\n",
    "print(f\"Some sample entropies: {entropy[:10]}\")\n",
    "print(f\"Average entropy: {entropy.mean()}\")\n",
    "\n",
    "# Compare with entropy of random logits\n",
    "random_logits = torch.randn_like(targets['policy'])\n",
    "random_cross_entropy = F.cross_entropy(random_logits, targets['policy'])\n",
    "\n",
    "print(f\"KL divergence between target and random logits: {random_cross_entropy - entropy.mean()}\")\n",
    "\n",
    "# Compare with entropy of uniform logits\n",
    "uniform_logits = torch.zeros_like(targets['policy'])\n",
    "uniform_cross_entropy = F.cross_entropy(uniform_logits, targets['policy'])\n",
    "\n",
    "print(f\"KL divergence between target and uniform logits: {uniform_cross_entropy - entropy.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data_structures import ReplayBuffer\n",
    "\n",
    "for tensor_mapping, states, targets, data in zip([MLPTensorMapping, TokenizedTensorMapping], [mlp_states, tokenized_states], [mlp_targets, tokenized_targets], [mlp_data, tokenized_data]):\n",
    "    buffer = ReplayBuffer(max_size=len(examples))\n",
    "    buffer.add(states, targets, data)\n",
    "    artifact_name = f'buffer_minimax_{tensor_mapping.__name__}'\n",
    "    path = f'experiments/experimenting_with_model_architectures_in_tic_tac_toe/data/{artifact_name}.pt'\n",
    "    buffer.save_to_file(path)\n",
    "# buffer.save_to_wandb(\n",
    "#     artifact_name=artifact_name,\n",
    "#     project='AlphaZero-TicTacToe',\n",
    "#     description=f'Training data for tic tac toe and {tensor_mapping.__name__} tensor mapping created by Minimax agent'\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
