{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes\n"
     ]
    }
   ],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applications.dots_and_boxes.NNmodels.transformer import TransformerInitParams\n",
    "from applications.dots_and_boxes.encoder import DABMiddleGroundTensorMapping\n",
    "from core.implementations import AlphaZeroConfig\n",
    "import torch\n",
    "\n",
    "# Initialize parameters\n",
    "\n",
    "## Model parameters\n",
    "model_type = 'transformer'\n",
    "model_params: TransformerInitParams = {\n",
    "    'num_rows': 5,\n",
    "    'num_cols': 5,\n",
    "    'embed_dim': 128,\n",
    "    'feedforward_dim': 512,\n",
    "    'num_heads': 4,\n",
    "    'attention_layers': 4\n",
    "}\n",
    "device = torch.device('mps')\n",
    "model_name = 'dots_and_boxes_transformer'\n",
    "tensor_mapping = DABMiddleGroundTensorMapping\n",
    "\n",
    "## Initialize new model\n",
    "# load_model = None\n",
    "# load_model_params = {}\n",
    "load_model = 'from_wandb'\n",
    "load_model_params = {\n",
    "    'project': 'AlphaZero-DotsAndBoxes',\n",
    "    'model_name': model_name,\n",
    "    'model_version': 'v16'\n",
    "}\n",
    "\n",
    "## Optimizer parameters\n",
    "optimizer_type = 'adam'\n",
    "optimizer_params = {\n",
    "    'lr': 1e-2,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'eps': 1e-8,\n",
    "    'weight_decay': 1e-4,\n",
    "    'amsgrad': False\n",
    "}\n",
    "\n",
    "## Learning scheduler parameters\n",
    "lr_scheduler_type = 'plateau'\n",
    "lr_scheduler_params = {\n",
    "    'factor': 0.5,\n",
    "    'patience': 100,\n",
    "    'cooldown': 100,\n",
    "    'min_lr': 1e-6\n",
    "}\n",
    "\n",
    "## Training parameters\n",
    "# training_method = 'supervised'\n",
    "# trainer_params = {}\n",
    "# training_params = {\n",
    "#     'epochs': 100,\n",
    "#     'batch_size': 256,\n",
    "#     'eval_freq': 25,\n",
    "#     'checkpoint_freq': 50,\n",
    "#     'mask_illegal_moves': False,\n",
    "#     'mask_value': -20.0, # Doesn't matter when mask_illegal_moves is False\n",
    "#     'checkpoint_dir': 'checkpoints',\n",
    "#     'start_at': 1\n",
    "# }\n",
    "training_method = 'self_play'\n",
    "alphazero_config = AlphaZeroConfig(\n",
    "    exploration_constant=1.0,\n",
    "    dirichlet_alpha=0.3,\n",
    "    dirichlet_epsilon=0.25,\n",
    "    temperature=1.0\n",
    ")\n",
    "alphazero_eval_config = AlphaZeroConfig(\n",
    "    exploration_constant=1.0,\n",
    "    dirichlet_alpha=0.0,\n",
    "    dirichlet_epsilon=0.0,\n",
    "    temperature=0.0\n",
    ")\n",
    "trainer_params = {\n",
    "    'value_softness': 1.0\n",
    "}\n",
    "training_params = {\n",
    "    'num_iterations': 500,\n",
    "    'games_per_iteration': 10,\n",
    "    'batch_size': 256,\n",
    "    'steps_per_iteration': 100,\n",
    "    'num_simulations': 100,\n",
    "    'checkpoint_frequency': 20,\n",
    "    'tree_search_params': alphazero_config,\n",
    "    'tree_search_eval_params': alphazero_eval_config,\n",
    "    'start_iteration': 150\n",
    "}\n",
    "\n",
    "## Load replay buffer from wandb\n",
    "load_replay_buffer = 'from_wandb'\n",
    "load_replay_buffer_params = {\n",
    "    'project': 'AlphaZero-DotsAndBoxes',\n",
    "    'artifact_name': f'{model_name}_replay_buffer',\n",
    "    'artifact_version': 'v5'\n",
    "}\n",
    "# load_replay_buffer = None\n",
    "# load_replay_buffer_params = {\n",
    "#     'max_size': 10**4\n",
    "# }\n",
    "# load_replay_buffer = 'from_file'\n",
    "# buffer_type = 'mcts' # used in path name below\n",
    "# load_replay_buffer_params = {\n",
    "#     'path': f'applications/dots_and_boxes/training_data/dots_and_boxes_{model_params[\"num_rows\"]}x{model_params[\"num_cols\"]}_{tensor_mapping.__name__}_{buffer_type}.pkl',\n",
    "#     'device': device\n",
    "# }\n",
    "# load_replay_buffer = 'from_wandb'\n",
    "# load_replay_buffer_params = {\n",
    "#     'project': 'AlphaZero-DotsAndBoxes',\n",
    "#     'artifact_name': f'dots_and_boxes_{model_params[\"num_rows\"]}x{model_params[\"num_cols\"]}_SimpleTensorMapping_minimax',\n",
    "#     'artifact_version': 'latest'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meohjelle\u001b[0m (\u001b[33meigenway\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes/wandb/run-20250330_141025-4qihqmj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/4qihqmj6' target=\"_blank\">Self-play Transformer</a></strong> to <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/4qihqmj6' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/4qihqmj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb run\n",
    "import wandb\n",
    "\n",
    "run_name = 'Self-play Transformer'\n",
    "notes = 'Transformer model with middle ground encoding training with self-play on 5 x 5 board.'\n",
    "\n",
    "config = {\n",
    "    'model_type': model_type,\n",
    "    'model_params': model_params,\n",
    "    'optimizer_type': optimizer_type,\n",
    "    'optimizer_params': optimizer_params,\n",
    "    'lr_scheduler_type': lr_scheduler_type,\n",
    "    'lr_scheduler_params': lr_scheduler_params,\n",
    "    'training_method': training_method,\n",
    "    'trainer_params': trainer_params,\n",
    "    'training_params': training_params\n",
    "}\n",
    "\n",
    "# run = wandb.init(\n",
    "#     project='AlphaZero-DotsAndBoxes',\n",
    "#     name=run_name,\n",
    "#     config=config,\n",
    "#     notes=notes,\n",
    "#     group=f'{training_method} training on {model_params[\"num_rows\"]}x{model_params[\"num_cols\"]} board'\n",
    "# )\n",
    "# run = None\n",
    "run_id='4qihqmj6'\n",
    "run = wandb.init(\n",
    "    project='AlphaZero-DotsAndBoxes',\n",
    "    id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "/Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes/core/model_interface.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact dots_and_boxes_transformer_replay_buffer:v5, 81.26MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.2\n",
      "/Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes/core/data_structures.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get best score for dots_and_boxes_transformer from wandb: HTTP 400: artifacts must be specified as 'collection:alias'\n",
      "Using initial best score of -inf.\n",
      "\n",
      "Iteration 151/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 151 summary:\n",
      "Average loss: 3.7431\n",
      "Average policy_loss: 2.6480\n",
      "Average value_loss: 1.0951\n",
      "Replay buffer size: 73810\n",
      "Time taken: 945.3s\n",
      "\n",
      "Iteration 152/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 152 summary:\n",
      "Average loss: 3.7357\n",
      "Average policy_loss: 2.6405\n",
      "Average value_loss: 1.0953\n",
      "Replay buffer size: 74420\n",
      "Time taken: 986.2s\n",
      "\n",
      "Iteration 153/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 153 summary:\n",
      "Average loss: 3.7303\n",
      "Average policy_loss: 2.6316\n",
      "Average value_loss: 1.0987\n",
      "Replay buffer size: 75030\n",
      "Time taken: 990.2s\n",
      "\n",
      "Iteration 154/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 154 summary:\n",
      "Average loss: 3.7536\n",
      "Average policy_loss: 2.6400\n",
      "Average value_loss: 1.1136\n",
      "Replay buffer size: 75640\n",
      "Time taken: 930.0s\n",
      "\n",
      "Iteration 155/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "New best score: -30.00%\n",
      "\n",
      "Iteration 155 summary:\n",
      "Average loss: 3.7446\n",
      "Average policy_loss: 2.6432\n",
      "Average value_loss: 1.1014\n",
      "Replay buffer size: 76250\n",
      "Time taken: 1780.0s\n",
      "\n",
      "Iteration 156/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 156 summary:\n",
      "Average loss: 3.7483\n",
      "Average policy_loss: 2.6418\n",
      "Average value_loss: 1.1065\n",
      "Replay buffer size: 76860\n",
      "Time taken: 902.9s\n",
      "\n",
      "Iteration 157/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 157 summary:\n",
      "Average loss: 3.7457\n",
      "Average policy_loss: 2.6332\n",
      "Average value_loss: 1.1125\n",
      "Replay buffer size: 77470\n",
      "Time taken: 903.5s\n",
      "\n",
      "Iteration 158/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 158 summary:\n",
      "Average loss: 3.7481\n",
      "Average policy_loss: 2.6398\n",
      "Average value_loss: 1.1083\n",
      "Replay buffer size: 78080\n",
      "Time taken: 894.0s\n",
      "\n",
      "Iteration 159/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 159 summary:\n",
      "Average loss: 3.7530\n",
      "Average policy_loss: 2.6427\n",
      "Average value_loss: 1.1103\n",
      "Replay buffer size: 78690\n",
      "Time taken: 898.3s\n",
      "\n",
      "Iteration 160/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 20.00%, Draw rate = 0.00%, Loss rate = 80.00%\n",
      "\n",
      "Iteration 160 summary:\n",
      "Average loss: 3.7421\n",
      "Average policy_loss: 2.6367\n",
      "Average value_loss: 1.1054\n",
      "Replay buffer size: 79300\n",
      "Time taken: 1768.6s\n",
      "\n",
      "Iteration 161/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 161 summary:\n",
      "Average loss: 3.7251\n",
      "Average policy_loss: 2.6270\n",
      "Average value_loss: 1.0981\n",
      "Replay buffer size: 79910\n",
      "Time taken: 901.3s\n",
      "\n",
      "Iteration 162/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 162 summary:\n",
      "Average loss: 3.7391\n",
      "Average policy_loss: 2.6242\n",
      "Average value_loss: 1.1149\n",
      "Replay buffer size: 80520\n",
      "Time taken: 893.8s\n",
      "\n",
      "Iteration 163/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 163 summary:\n",
      "Average loss: 3.7475\n",
      "Average policy_loss: 2.6381\n",
      "Average value_loss: 1.1094\n",
      "Replay buffer size: 81130\n",
      "Time taken: 902.8s\n",
      "\n",
      "Iteration 164/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 164 summary:\n",
      "Average loss: 3.7566\n",
      "Average policy_loss: 2.6470\n",
      "Average value_loss: 1.1096\n",
      "Replay buffer size: 81740\n",
      "Time taken: 905.0s\n",
      "\n",
      "Iteration 165/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 50.00%, Draw rate = 0.00%, Loss rate = 50.00%\n",
      "New best score: 0.00%\n",
      "\n",
      "Iteration 165 summary:\n",
      "Average loss: 3.7598\n",
      "Average policy_loss: 2.6470\n",
      "Average value_loss: 1.1128\n",
      "Replay buffer size: 82350\n",
      "Time taken: 1806.7s\n",
      "\n",
      "Iteration 166/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 166 summary:\n",
      "Average loss: 3.7570\n",
      "Average policy_loss: 2.6438\n",
      "Average value_loss: 1.1131\n",
      "Replay buffer size: 82960\n",
      "Time taken: 925.9s\n",
      "\n",
      "Iteration 167/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 167 summary:\n",
      "Average loss: 3.7607\n",
      "Average policy_loss: 2.6433\n",
      "Average value_loss: 1.1174\n",
      "Replay buffer size: 83570\n",
      "Time taken: 920.7s\n",
      "\n",
      "Iteration 168/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 168 summary:\n",
      "Average loss: 3.7609\n",
      "Average policy_loss: 2.6470\n",
      "Average value_loss: 1.1139\n",
      "Replay buffer size: 84180\n",
      "Time taken: 986.7s\n",
      "\n",
      "Iteration 169/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 169 summary:\n",
      "Average loss: 5.8979\n",
      "Average policy_loss: 4.7659\n",
      "Average value_loss: 1.1319\n",
      "Replay buffer size: 84790\n",
      "Time taken: 1001.8s\n",
      "\n",
      "Iteration 170/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 80.00%, Draw rate = 0.00%, Loss rate = 20.00%\n",
      "New best score: 60.00%\n",
      "\n",
      "Iteration 170 summary:\n",
      "Average loss: 4.0076\n",
      "Average policy_loss: 2.8532\n",
      "Average value_loss: 1.1544\n",
      "Replay buffer size: 85400\n",
      "Time taken: 1962.3s\n",
      "\n",
      "Iteration 171/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 171 summary:\n",
      "Average loss: 4.0004\n",
      "Average policy_loss: 2.8352\n",
      "Average value_loss: 1.1651\n",
      "Replay buffer size: 86010\n",
      "Time taken: 952.6s\n",
      "\n",
      "Iteration 172/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 172 summary:\n",
      "Average loss: 3.9761\n",
      "Average policy_loss: 2.8220\n",
      "Average value_loss: 1.1541\n",
      "Replay buffer size: 86620\n",
      "Time taken: 956.9s\n",
      "\n",
      "Iteration 173/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 173 summary:\n",
      "Average loss: 3.9858\n",
      "Average policy_loss: 2.8230\n",
      "Average value_loss: 1.1628\n",
      "Replay buffer size: 87230\n",
      "Time taken: 933.9s\n",
      "\n",
      "Iteration 174/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 174 summary:\n",
      "Average loss: 3.9626\n",
      "Average policy_loss: 2.8108\n",
      "Average value_loss: 1.1518\n",
      "Replay buffer size: 87840\n",
      "Time taken: 930.3s\n",
      "\n",
      "Iteration 175/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 40.00%, Draw rate = 0.00%, Loss rate = 60.00%\n",
      "\n",
      "Iteration 175 summary:\n",
      "Average loss: 3.9604\n",
      "Average policy_loss: 2.8062\n",
      "Average value_loss: 1.1542\n",
      "Replay buffer size: 88450\n",
      "Time taken: 1848.7s\n",
      "\n",
      "Iteration 176/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 176 summary:\n",
      "Average loss: 3.9305\n",
      "Average policy_loss: 2.8156\n",
      "Average value_loss: 1.1149\n",
      "Replay buffer size: 89060\n",
      "Time taken: 940.2s\n",
      "\n",
      "Iteration 177/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 177 summary:\n",
      "Average loss: 3.9263\n",
      "Average policy_loss: 2.8157\n",
      "Average value_loss: 1.1106\n",
      "Replay buffer size: 89670\n",
      "Time taken: 963.9s\n",
      "\n",
      "Iteration 178/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 178 summary:\n",
      "Average loss: 3.9972\n",
      "Average policy_loss: 2.8814\n",
      "Average value_loss: 1.1158\n",
      "Replay buffer size: 90280\n",
      "Time taken: 978.8s\n",
      "\n",
      "Iteration 179/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 179 summary:\n",
      "Average loss: 4.5068\n",
      "Average policy_loss: 3.3880\n",
      "Average value_loss: 1.1188\n",
      "Replay buffer size: 90890\n",
      "Time taken: 856.7s\n",
      "\n",
      "Iteration 180/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 20.00%, Draw rate = 0.00%, Loss rate = 80.00%\n",
      "\n",
      "Iteration 180 summary:\n",
      "Average loss: 4.9240\n",
      "Average policy_loss: 3.8038\n",
      "Average value_loss: 1.1202\n",
      "Replay buffer size: 91500\n",
      "Time taken: 1596.7s\n",
      "\n",
      "Iteration 181/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 181 summary:\n",
      "Average loss: 3.9878\n",
      "Average policy_loss: 2.8731\n",
      "Average value_loss: 1.1147\n",
      "Replay buffer size: 92110\n",
      "Time taken: 807.2s\n",
      "\n",
      "Iteration 182/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 182 summary:\n",
      "Average loss: 3.9429\n",
      "Average policy_loss: 2.8232\n",
      "Average value_loss: 1.1197\n",
      "Replay buffer size: 92720\n",
      "Time taken: 804.1s\n",
      "\n",
      "Iteration 183/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 183 summary:\n",
      "Average loss: 3.9434\n",
      "Average policy_loss: 2.8199\n",
      "Average value_loss: 1.1234\n",
      "Replay buffer size: 93330\n",
      "Time taken: 805.5s\n",
      "\n",
      "Iteration 184/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 184 summary:\n",
      "Average loss: 3.9372\n",
      "Average policy_loss: 2.8144\n",
      "Average value_loss: 1.1228\n",
      "Replay buffer size: 93940\n",
      "Time taken: 811.7s\n",
      "\n",
      "Iteration 185/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 40.00%, Draw rate = 0.00%, Loss rate = 60.00%\n",
      "\n",
      "Iteration 185 summary:\n",
      "Average loss: 3.9283\n",
      "Average policy_loss: 2.8070\n",
      "Average value_loss: 1.1213\n",
      "Replay buffer size: 94550\n",
      "Time taken: 1561.6s\n",
      "\n",
      "Iteration 186/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 186 summary:\n",
      "Average loss: 3.9304\n",
      "Average policy_loss: 2.8123\n",
      "Average value_loss: 1.1181\n",
      "Replay buffer size: 95160\n",
      "Time taken: 809.6s\n",
      "\n",
      "Iteration 187/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 187 summary:\n",
      "Average loss: 3.9400\n",
      "Average policy_loss: 2.8216\n",
      "Average value_loss: 1.1185\n",
      "Replay buffer size: 95770\n",
      "Time taken: 807.5s\n",
      "\n",
      "Iteration 188/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 188 summary:\n",
      "Average loss: 3.9328\n",
      "Average policy_loss: 2.8119\n",
      "Average value_loss: 1.1210\n",
      "Replay buffer size: 96380\n",
      "Time taken: 814.5s\n",
      "\n",
      "Iteration 189/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 189 summary:\n",
      "Average loss: 3.9391\n",
      "Average policy_loss: 2.8196\n",
      "Average value_loss: 1.1196\n",
      "Replay buffer size: 96990\n",
      "Time taken: 808.8s\n",
      "\n",
      "Iteration 190/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 15.00%, Draw rate = 0.00%, Loss rate = 85.00%\n",
      "\n",
      "Iteration 190 summary:\n",
      "Average loss: 3.9280\n",
      "Average policy_loss: 2.8058\n",
      "Average value_loss: 1.1222\n",
      "Replay buffer size: 97600\n",
      "Time taken: 1528.7s\n",
      "\n",
      "Iteration 191/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 191 summary:\n",
      "Average loss: 3.9495\n",
      "Average policy_loss: 2.8163\n",
      "Average value_loss: 1.1332\n",
      "Replay buffer size: 98210\n",
      "Time taken: 804.7s\n",
      "\n",
      "Iteration 192/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 192 summary:\n",
      "Average loss: 3.9375\n",
      "Average policy_loss: 2.8120\n",
      "Average value_loss: 1.1255\n",
      "Replay buffer size: 98820\n",
      "Time taken: 814.0s\n",
      "\n",
      "Iteration 193/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 193 summary:\n",
      "Average loss: 3.9401\n",
      "Average policy_loss: 2.8167\n",
      "Average value_loss: 1.1234\n",
      "Replay buffer size: 99430\n",
      "Time taken: 807.6s\n",
      "\n",
      "Iteration 194/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 194 summary:\n",
      "Average loss: 3.9574\n",
      "Average policy_loss: 2.8254\n",
      "Average value_loss: 1.1320\n",
      "Replay buffer size: 100000\n",
      "Time taken: 808.5s\n",
      "\n",
      "Iteration 195/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 45.00%, Draw rate = 0.00%, Loss rate = 55.00%\n",
      "\n",
      "Iteration 195 summary:\n",
      "Average loss: 3.9421\n",
      "Average policy_loss: 2.8107\n",
      "Average value_loss: 1.1314\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1553.9s\n",
      "\n",
      "Iteration 196/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 196 summary:\n",
      "Average loss: 3.9520\n",
      "Average policy_loss: 2.8073\n",
      "Average value_loss: 1.1447\n",
      "Replay buffer size: 100000\n",
      "Time taken: 813.1s\n",
      "\n",
      "Iteration 197/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 197 summary:\n",
      "Average loss: 3.9417\n",
      "Average policy_loss: 2.8051\n",
      "Average value_loss: 1.1366\n",
      "Replay buffer size: 100000\n",
      "Time taken: 799.5s\n",
      "\n",
      "Iteration 198/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 198 summary:\n",
      "Average loss: 3.9389\n",
      "Average policy_loss: 2.8051\n",
      "Average value_loss: 1.1337\n",
      "Replay buffer size: 100000\n",
      "Time taken: 801.9s\n",
      "\n",
      "Iteration 199/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 199 summary:\n",
      "Average loss: 3.9377\n",
      "Average policy_loss: 2.7936\n",
      "Average value_loss: 1.1442\n",
      "Replay buffer size: 100000\n",
      "Time taken: 804.4s\n",
      "\n",
      "Iteration 200/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "New best score: 80.00%\n",
      "\n",
      "Iteration 200 summary:\n",
      "Average loss: 3.9395\n",
      "Average policy_loss: 2.7981\n",
      "Average value_loss: 1.1414\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1657.8s\n",
      "\n",
      "Iteration 201/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 201 summary:\n",
      "Average loss: 3.9256\n",
      "Average policy_loss: 2.8047\n",
      "Average value_loss: 1.1209\n",
      "Replay buffer size: 100000\n",
      "Time taken: 792.0s\n",
      "\n",
      "Iteration 202/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 202 summary:\n",
      "Average loss: 3.9269\n",
      "Average policy_loss: 2.8035\n",
      "Average value_loss: 1.1233\n",
      "Replay buffer size: 100000\n",
      "Time taken: 813.3s\n",
      "\n",
      "Iteration 203/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 203 summary:\n",
      "Average loss: 3.9271\n",
      "Average policy_loss: 2.8009\n",
      "Average value_loss: 1.1262\n",
      "Replay buffer size: 100000\n",
      "Time taken: 810.7s\n",
      "\n",
      "Iteration 204/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 204 summary:\n",
      "Average loss: 3.9432\n",
      "Average policy_loss: 2.8063\n",
      "Average value_loss: 1.1370\n",
      "Replay buffer size: 100000\n",
      "Time taken: 813.4s\n",
      "\n",
      "Iteration 205/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 205 summary:\n",
      "Average loss: 3.9350\n",
      "Average policy_loss: 2.7981\n",
      "Average value_loss: 1.1369\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1668.2s\n",
      "\n",
      "Iteration 206/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 206 summary:\n",
      "Average loss: 3.9361\n",
      "Average policy_loss: 2.8058\n",
      "Average value_loss: 1.1302\n",
      "Replay buffer size: 100000\n",
      "Time taken: 802.7s\n",
      "\n",
      "Iteration 207/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 207 summary:\n",
      "Average loss: 3.9138\n",
      "Average policy_loss: 2.7848\n",
      "Average value_loss: 1.1291\n",
      "Replay buffer size: 100000\n",
      "Time taken: 797.4s\n",
      "\n",
      "Iteration 208/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 208 summary:\n",
      "Average loss: 3.9242\n",
      "Average policy_loss: 2.8047\n",
      "Average value_loss: 1.1195\n",
      "Replay buffer size: 100000\n",
      "Time taken: 805.1s\n",
      "\n",
      "Iteration 209/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 209 summary:\n",
      "Average loss: 3.9419\n",
      "Average policy_loss: 2.8008\n",
      "Average value_loss: 1.1410\n",
      "Replay buffer size: 100000\n",
      "Time taken: 804.8s\n",
      "\n",
      "Iteration 210/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 75.00%, Draw rate = 0.00%, Loss rate = 25.00%\n",
      "\n",
      "Iteration 210 summary:\n",
      "Average loss: 3.9348\n",
      "Average policy_loss: 2.7956\n",
      "Average value_loss: 1.1393\n",
      "Replay buffer size: 100000\n",
      "Time taken: 2552.8s\n",
      "\n",
      "Iteration 211/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 211 summary:\n",
      "Average loss: 3.9342\n",
      "Average policy_loss: 2.7926\n",
      "Average value_loss: 1.1416\n",
      "Replay buffer size: 100000\n",
      "Time taken: 804.1s\n",
      "\n",
      "Iteration 212/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 212 summary:\n",
      "Average loss: 3.9225\n",
      "Average policy_loss: 2.7876\n",
      "Average value_loss: 1.1349\n",
      "Replay buffer size: 100000\n",
      "Time taken: 819.7s\n",
      "\n",
      "Iteration 213/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 213 summary:\n",
      "Average loss: 3.9295\n",
      "Average policy_loss: 2.7911\n",
      "Average value_loss: 1.1384\n",
      "Replay buffer size: 100000\n",
      "Time taken: 829.1s\n",
      "\n",
      "Iteration 214/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 214 summary:\n",
      "Average loss: 3.9235\n",
      "Average policy_loss: 2.7931\n",
      "Average value_loss: 1.1304\n",
      "Replay buffer size: 100000\n",
      "Time taken: 814.6s\n",
      "\n",
      "Iteration 215/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 100.00%, Draw rate = 0.00%, Loss rate = 0.00%\n",
      "New best score: 100.00%\n",
      "\n",
      "Iteration 215 summary:\n",
      "Average loss: 3.9269\n",
      "Average policy_loss: 2.7943\n",
      "Average value_loss: 1.1326\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1686.4s\n",
      "\n",
      "Iteration 216/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 216 summary:\n",
      "Average loss: 3.9363\n",
      "Average policy_loss: 2.7887\n",
      "Average value_loss: 1.1476\n",
      "Replay buffer size: 100000\n",
      "Time taken: 814.1s\n",
      "\n",
      "Iteration 217/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 217 summary:\n",
      "Average loss: 3.9205\n",
      "Average policy_loss: 2.7780\n",
      "Average value_loss: 1.1425\n",
      "Replay buffer size: 100000\n",
      "Time taken: 803.2s\n",
      "\n",
      "Iteration 218/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 218 summary:\n",
      "Average loss: 3.9281\n",
      "Average policy_loss: 2.7866\n",
      "Average value_loss: 1.1415\n",
      "Replay buffer size: 100000\n",
      "Time taken: 811.6s\n",
      "\n",
      "Iteration 219/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 219 summary:\n",
      "Average loss: 3.9201\n",
      "Average policy_loss: 2.7756\n",
      "Average value_loss: 1.1445\n",
      "Replay buffer size: 100000\n",
      "Time taken: 801.9s\n",
      "\n",
      "Iteration 220/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 220 summary:\n",
      "Average loss: 3.9197\n",
      "Average policy_loss: 2.7775\n",
      "Average value_loss: 1.1421\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1710.2s\n",
      "\n",
      "Iteration 221/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 221 summary:\n",
      "Average loss: 3.9284\n",
      "Average policy_loss: 2.7877\n",
      "Average value_loss: 1.1407\n",
      "Replay buffer size: 100000\n",
      "Time taken: 825.6s\n",
      "\n",
      "Iteration 222/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 222 summary:\n",
      "Average loss: 3.9220\n",
      "Average policy_loss: 2.7742\n",
      "Average value_loss: 1.1478\n",
      "Replay buffer size: 100000\n",
      "Time taken: 799.3s\n",
      "\n",
      "Iteration 223/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 223 summary:\n",
      "Average loss: 3.9123\n",
      "Average policy_loss: 2.7677\n",
      "Average value_loss: 1.1446\n",
      "Replay buffer size: 100000\n",
      "Time taken: 18709.7s\n",
      "\n",
      "Iteration 224/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 224 summary:\n",
      "Average loss: 3.9283\n",
      "Average policy_loss: 2.7773\n",
      "Average value_loss: 1.1511\n",
      "Replay buffer size: 100000\n",
      "Time taken: 13141.2s\n",
      "\n",
      "Iteration 225/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 75.00%, Draw rate = 0.00%, Loss rate = 25.00%\n",
      "\n",
      "Iteration 225 summary:\n",
      "Average loss: 3.9119\n",
      "Average policy_loss: 2.7674\n",
      "Average value_loss: 1.1446\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1666.9s\n",
      "\n",
      "Iteration 226/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 226 summary:\n",
      "Average loss: 3.9165\n",
      "Average policy_loss: 2.7719\n",
      "Average value_loss: 1.1446\n",
      "Replay buffer size: 100000\n",
      "Time taken: 807.3s\n",
      "\n",
      "Iteration 227/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 227 summary:\n",
      "Average loss: 3.9125\n",
      "Average policy_loss: 2.7720\n",
      "Average value_loss: 1.1404\n",
      "Replay buffer size: 100000\n",
      "Time taken: 805.5s\n",
      "\n",
      "Iteration 228/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 228 summary:\n",
      "Average loss: 3.9202\n",
      "Average policy_loss: 2.7746\n",
      "Average value_loss: 1.1456\n",
      "Replay buffer size: 100000\n",
      "Time taken: 815.1s\n",
      "\n",
      "Iteration 229/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 229 summary:\n",
      "Average loss: 3.9069\n",
      "Average policy_loss: 2.7622\n",
      "Average value_loss: 1.1446\n",
      "Replay buffer size: 100000\n",
      "Time taken: 2604.8s\n",
      "\n",
      "Iteration 230/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 100.00%, Draw rate = 0.00%, Loss rate = 0.00%\n",
      "\n",
      "Iteration 230 summary:\n",
      "Average loss: 3.9020\n",
      "Average policy_loss: 2.7625\n",
      "Average value_loss: 1.1395\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1965.5s\n",
      "\n",
      "Iteration 231/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 231 summary:\n",
      "Average loss: 3.9015\n",
      "Average policy_loss: 2.7598\n",
      "Average value_loss: 1.1417\n",
      "Replay buffer size: 100000\n",
      "Time taken: 3124.0s\n",
      "\n",
      "Iteration 232/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 232 summary:\n",
      "Average loss: 3.9140\n",
      "Average policy_loss: 2.7684\n",
      "Average value_loss: 1.1455\n",
      "Replay buffer size: 100000\n",
      "Time taken: 855.2s\n",
      "\n",
      "Iteration 233/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 233 summary:\n",
      "Average loss: 3.9045\n",
      "Average policy_loss: 2.7557\n",
      "Average value_loss: 1.1488\n",
      "Replay buffer size: 100000\n",
      "Time taken: 847.9s\n",
      "\n",
      "Iteration 234/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 234 summary:\n",
      "Average loss: 3.9149\n",
      "Average policy_loss: 2.7710\n",
      "Average value_loss: 1.1439\n",
      "Replay buffer size: 100000\n",
      "Time taken: 845.8s\n",
      "\n",
      "Iteration 235/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 235 summary:\n",
      "Average loss: 3.9043\n",
      "Average policy_loss: 2.7546\n",
      "Average value_loss: 1.1496\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1742.0s\n",
      "\n",
      "Iteration 236/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 236 summary:\n",
      "Average loss: 3.9164\n",
      "Average policy_loss: 2.7641\n",
      "Average value_loss: 1.1523\n",
      "Replay buffer size: 100000\n",
      "Time taken: 855.1s\n",
      "\n",
      "Iteration 237/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 237 summary:\n",
      "Average loss: 3.9020\n",
      "Average policy_loss: 2.7553\n",
      "Average value_loss: 1.1467\n",
      "Replay buffer size: 100000\n",
      "Time taken: 854.8s\n",
      "\n",
      "Iteration 238/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 238 summary:\n",
      "Average loss: 3.9096\n",
      "Average policy_loss: 2.7538\n",
      "Average value_loss: 1.1558\n",
      "Replay buffer size: 100000\n",
      "Time taken: 845.0s\n",
      "\n",
      "Iteration 239/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 239 summary:\n",
      "Average loss: 3.9114\n",
      "Average policy_loss: 2.7560\n",
      "Average value_loss: 1.1554\n",
      "Replay buffer size: 100000\n",
      "Time taken: 856.7s\n",
      "\n",
      "Iteration 240/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 80.00%, Draw rate = 0.00%, Loss rate = 20.00%\n",
      "\n",
      "Iteration 240 summary:\n",
      "Average loss: 3.9072\n",
      "Average policy_loss: 2.7513\n",
      "Average value_loss: 1.1559\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1769.3s\n",
      "\n",
      "Iteration 241/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 241 summary:\n",
      "Average loss: 3.9081\n",
      "Average policy_loss: 2.7455\n",
      "Average value_loss: 1.1626\n",
      "Replay buffer size: 100000\n",
      "Time taken: 857.1s\n",
      "\n",
      "Iteration 242/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 242 summary:\n",
      "Average loss: 3.8994\n",
      "Average policy_loss: 2.7520\n",
      "Average value_loss: 1.1474\n",
      "Replay buffer size: 100000\n",
      "Time taken: 857.3s\n",
      "\n",
      "Iteration 243/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 243 summary:\n",
      "Average loss: 3.8952\n",
      "Average policy_loss: 2.7462\n",
      "Average value_loss: 1.1490\n",
      "Replay buffer size: 100000\n",
      "Time taken: 858.7s\n",
      "\n",
      "Iteration 244/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 244 summary:\n",
      "Average loss: 3.8967\n",
      "Average policy_loss: 2.7439\n",
      "Average value_loss: 1.1528\n",
      "Replay buffer size: 100000\n",
      "Time taken: 864.1s\n",
      "\n",
      "Iteration 245/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 245 summary:\n",
      "Average loss: 3.9125\n",
      "Average policy_loss: 2.7500\n",
      "Average value_loss: 1.1625\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1778.7s\n",
      "\n",
      "Iteration 246/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 246 summary:\n",
      "Average loss: 3.9095\n",
      "Average policy_loss: 2.7486\n",
      "Average value_loss: 1.1608\n",
      "Replay buffer size: 100000\n",
      "Time taken: 858.7s\n",
      "\n",
      "Iteration 247/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 247 summary:\n",
      "Average loss: 3.8964\n",
      "Average policy_loss: 2.7364\n",
      "Average value_loss: 1.1600\n",
      "Replay buffer size: 100000\n",
      "Time taken: 868.8s\n",
      "\n",
      "Iteration 248/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 248 summary:\n",
      "Average loss: 3.8938\n",
      "Average policy_loss: 2.7353\n",
      "Average value_loss: 1.1585\n",
      "Replay buffer size: 100000\n",
      "Time taken: 875.8s\n",
      "\n",
      "Iteration 249/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 249 summary:\n",
      "Average loss: 3.9048\n",
      "Average policy_loss: 2.7425\n",
      "Average value_loss: 1.1623\n",
      "Replay buffer size: 100000\n",
      "Time taken: 866.0s\n",
      "\n",
      "Iteration 250/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 250 summary:\n",
      "Average loss: 3.9000\n",
      "Average policy_loss: 2.7316\n",
      "Average value_loss: 1.1684\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1786.9s\n",
      "\n",
      "Iteration 251/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 251 summary:\n",
      "Average loss: 3.8943\n",
      "Average policy_loss: 2.7375\n",
      "Average value_loss: 1.1568\n",
      "Replay buffer size: 100000\n",
      "Time taken: 854.7s\n",
      "\n",
      "Iteration 252/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 252 summary:\n",
      "Average loss: 3.9114\n",
      "Average policy_loss: 2.7424\n",
      "Average value_loss: 1.1690\n",
      "Replay buffer size: 100000\n",
      "Time taken: 873.1s\n",
      "\n",
      "Iteration 253/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 253 summary:\n",
      "Average loss: 3.9170\n",
      "Average policy_loss: 2.7375\n",
      "Average value_loss: 1.1794\n",
      "Replay buffer size: 100000\n",
      "Time taken: 857.6s\n",
      "\n",
      "Iteration 254/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 254 summary:\n",
      "Average loss: 3.9064\n",
      "Average policy_loss: 2.7280\n",
      "Average value_loss: 1.1783\n",
      "Replay buffer size: 100000\n",
      "Time taken: 847.0s\n",
      "\n",
      "Iteration 255/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 255 summary:\n",
      "Average loss: 3.9079\n",
      "Average policy_loss: 2.7216\n",
      "Average value_loss: 1.1862\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1774.2s\n",
      "\n",
      "Iteration 256/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 256 summary:\n",
      "Average loss: 3.9095\n",
      "Average policy_loss: 2.7305\n",
      "Average value_loss: 1.1791\n",
      "Replay buffer size: 100000\n",
      "Time taken: 864.7s\n",
      "\n",
      "Iteration 257/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 257 summary:\n",
      "Average loss: 3.9026\n",
      "Average policy_loss: 2.7203\n",
      "Average value_loss: 1.1823\n",
      "Replay buffer size: 100000\n",
      "Time taken: 863.0s\n",
      "\n",
      "Iteration 258/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 258 summary:\n",
      "Average loss: 3.9043\n",
      "Average policy_loss: 2.7236\n",
      "Average value_loss: 1.1807\n",
      "Replay buffer size: 100000\n",
      "Time taken: 860.2s\n",
      "\n",
      "Iteration 259/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 259 summary:\n",
      "Average loss: 3.9302\n",
      "Average policy_loss: 2.7377\n",
      "Average value_loss: 1.1925\n",
      "Replay buffer size: 100000\n",
      "Time taken: 860.9s\n",
      "\n",
      "Iteration 260/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 95.00%, Draw rate = 0.00%, Loss rate = 5.00%\n",
      "\n",
      "Iteration 260 summary:\n",
      "Average loss: 3.9006\n",
      "Average policy_loss: 2.7127\n",
      "Average value_loss: 1.1879\n",
      "Replay buffer size: 100000\n",
      "Time taken: 2317.8s\n",
      "\n",
      "Iteration 261/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.010000\n",
      "\n",
      "Iteration 261 summary:\n",
      "Average loss: 3.9183\n",
      "Average policy_loss: 2.7203\n",
      "Average value_loss: 1.1979\n",
      "Replay buffer size: 100000\n",
      "Time taken: 836.2s\n",
      "\n",
      "Iteration 262/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 262 summary:\n",
      "Average loss: 3.9167\n",
      "Average policy_loss: 2.7215\n",
      "Average value_loss: 1.1952\n",
      "Replay buffer size: 100000\n",
      "Time taken: 834.7s\n",
      "\n",
      "Iteration 263/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 263 summary:\n",
      "Average loss: 3.9194\n",
      "Average policy_loss: 2.7211\n",
      "Average value_loss: 1.1983\n",
      "Replay buffer size: 100000\n",
      "Time taken: 835.9s\n",
      "\n",
      "Iteration 264/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 264 summary:\n",
      "Average loss: 3.9079\n",
      "Average policy_loss: 2.7055\n",
      "Average value_loss: 1.2023\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.9s\n",
      "\n",
      "Iteration 265/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 75.00%, Draw rate = 0.00%, Loss rate = 25.00%\n",
      "\n",
      "Iteration 265 summary:\n",
      "Average loss: 3.9162\n",
      "Average policy_loss: 2.7110\n",
      "Average value_loss: 1.2052\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1688.0s\n",
      "\n",
      "Iteration 266/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 266 summary:\n",
      "Average loss: 3.9144\n",
      "Average policy_loss: 2.7141\n",
      "Average value_loss: 1.2003\n",
      "Replay buffer size: 100000\n",
      "Time taken: 837.3s\n",
      "\n",
      "Iteration 267/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 267 summary:\n",
      "Average loss: 3.9252\n",
      "Average policy_loss: 2.7135\n",
      "Average value_loss: 1.2117\n",
      "Replay buffer size: 100000\n",
      "Time taken: 837.3s\n",
      "\n",
      "Iteration 268/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 268 summary:\n",
      "Average loss: 3.9121\n",
      "Average policy_loss: 2.7067\n",
      "Average value_loss: 1.2054\n",
      "Replay buffer size: 100000\n",
      "Time taken: 830.0s\n",
      "\n",
      "Iteration 269/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 269 summary:\n",
      "Average loss: 3.9095\n",
      "Average policy_loss: 2.6999\n",
      "Average value_loss: 1.2096\n",
      "Replay buffer size: 100000\n",
      "Time taken: 831.1s\n",
      "\n",
      "Iteration 270/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 270 summary:\n",
      "Average loss: 3.9107\n",
      "Average policy_loss: 2.6990\n",
      "Average value_loss: 1.2117\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1680.7s\n",
      "\n",
      "Iteration 271/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 271 summary:\n",
      "Average loss: 3.9106\n",
      "Average policy_loss: 2.6988\n",
      "Average value_loss: 1.2118\n",
      "Replay buffer size: 100000\n",
      "Time taken: 823.8s\n",
      "\n",
      "Iteration 272/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 272 summary:\n",
      "Average loss: 3.8996\n",
      "Average policy_loss: 2.6882\n",
      "Average value_loss: 1.2114\n",
      "Replay buffer size: 100000\n",
      "Time taken: 828.4s\n",
      "\n",
      "Iteration 273/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 273 summary:\n",
      "Average loss: 3.9050\n",
      "Average policy_loss: 2.6902\n",
      "Average value_loss: 1.2148\n",
      "Replay buffer size: 100000\n",
      "Time taken: 822.6s\n",
      "\n",
      "Iteration 274/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 274 summary:\n",
      "Average loss: 3.9118\n",
      "Average policy_loss: 2.6874\n",
      "Average value_loss: 1.2244\n",
      "Replay buffer size: 100000\n",
      "Time taken: 820.3s\n",
      "\n",
      "Iteration 275/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 275 summary:\n",
      "Average loss: 3.9063\n",
      "Average policy_loss: 2.6899\n",
      "Average value_loss: 1.2165\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1789.6s\n",
      "\n",
      "Iteration 276/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 276 summary:\n",
      "Average loss: 3.9208\n",
      "Average policy_loss: 2.7027\n",
      "Average value_loss: 1.2182\n",
      "Replay buffer size: 100000\n",
      "Time taken: 7824.5s\n",
      "\n",
      "Iteration 277/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 277 summary:\n",
      "Average loss: 3.9006\n",
      "Average policy_loss: 2.6843\n",
      "Average value_loss: 1.2163\n",
      "Replay buffer size: 100000\n",
      "Time taken: 935.9s\n",
      "\n",
      "Iteration 278/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 278 summary:\n",
      "Average loss: 3.9115\n",
      "Average policy_loss: 2.6930\n",
      "Average value_loss: 1.2185\n",
      "Replay buffer size: 100000\n",
      "Time taken: 925.4s\n",
      "\n",
      "Iteration 279/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 279 summary:\n",
      "Average loss: 3.9191\n",
      "Average policy_loss: 2.6918\n",
      "Average value_loss: 1.2273\n",
      "Replay buffer size: 100000\n",
      "Time taken: 933.8s\n",
      "\n",
      "Iteration 280/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 280 summary:\n",
      "Average loss: 3.9134\n",
      "Average policy_loss: 2.6964\n",
      "Average value_loss: 1.2169\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1897.6s\n",
      "\n",
      "Iteration 281/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 281 summary:\n",
      "Average loss: 3.9077\n",
      "Average policy_loss: 2.6811\n",
      "Average value_loss: 1.2266\n",
      "Replay buffer size: 100000\n",
      "Time taken: 922.8s\n",
      "\n",
      "Iteration 282/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 282 summary:\n",
      "Average loss: 3.8969\n",
      "Average policy_loss: 2.6805\n",
      "Average value_loss: 1.2163\n",
      "Replay buffer size: 100000\n",
      "Time taken: 942.9s\n",
      "\n",
      "Iteration 283/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 283 summary:\n",
      "Average loss: 3.9108\n",
      "Average policy_loss: 2.6886\n",
      "Average value_loss: 1.2223\n",
      "Replay buffer size: 100000\n",
      "Time taken: 934.3s\n",
      "\n",
      "Iteration 284/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 284 summary:\n",
      "Average loss: 3.9068\n",
      "Average policy_loss: 2.6840\n",
      "Average value_loss: 1.2228\n",
      "Replay buffer size: 100000\n",
      "Time taken: 939.6s\n",
      "\n",
      "Iteration 285/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 285 summary:\n",
      "Average loss: 3.9090\n",
      "Average policy_loss: 2.6797\n",
      "Average value_loss: 1.2294\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1948.9s\n",
      "\n",
      "Iteration 286/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 286 summary:\n",
      "Average loss: 3.8439\n",
      "Average policy_loss: 2.6713\n",
      "Average value_loss: 1.1726\n",
      "Replay buffer size: 100000\n",
      "Time taken: 942.6s\n",
      "\n",
      "Iteration 287/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 287 summary:\n",
      "Average loss: 3.7246\n",
      "Average policy_loss: 2.6736\n",
      "Average value_loss: 1.0510\n",
      "Replay buffer size: 100000\n",
      "Time taken: 904.6s\n",
      "\n",
      "Iteration 288/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 288 summary:\n",
      "Average loss: 3.7217\n",
      "Average policy_loss: 2.6769\n",
      "Average value_loss: 1.0449\n",
      "Replay buffer size: 100000\n",
      "Time taken: 907.9s\n",
      "\n",
      "Iteration 289/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 289 summary:\n",
      "Average loss: 3.7216\n",
      "Average policy_loss: 2.6746\n",
      "Average value_loss: 1.0470\n",
      "Replay buffer size: 100000\n",
      "Time taken: 920.1s\n",
      "\n",
      "Iteration 290/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 45.00%, Draw rate = 0.00%, Loss rate = 55.00%\n",
      "\n",
      "Iteration 290 summary:\n",
      "Average loss: 3.7209\n",
      "Average policy_loss: 2.6678\n",
      "Average value_loss: 1.0531\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1811.8s\n",
      "\n",
      "Iteration 291/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 291 summary:\n",
      "Average loss: 3.7118\n",
      "Average policy_loss: 2.6566\n",
      "Average value_loss: 1.0552\n",
      "Replay buffer size: 100000\n",
      "Time taken: 927.8s\n",
      "\n",
      "Iteration 292/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 292 summary:\n",
      "Average loss: 3.7214\n",
      "Average policy_loss: 2.6707\n",
      "Average value_loss: 1.0507\n",
      "Replay buffer size: 100000\n",
      "Time taken: 960.7s\n",
      "\n",
      "Iteration 293/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 293 summary:\n",
      "Average loss: 3.7278\n",
      "Average policy_loss: 2.6770\n",
      "Average value_loss: 1.0508\n",
      "Replay buffer size: 100000\n",
      "Time taken: 927.8s\n",
      "\n",
      "Iteration 294/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 294 summary:\n",
      "Average loss: 3.7173\n",
      "Average policy_loss: 2.6668\n",
      "Average value_loss: 1.0505\n",
      "Replay buffer size: 100000\n",
      "Time taken: 920.2s\n",
      "\n",
      "Iteration 295/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 295 summary:\n",
      "Average loss: 3.7109\n",
      "Average policy_loss: 2.6558\n",
      "Average value_loss: 1.0551\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1790.3s\n",
      "\n",
      "Iteration 296/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 296 summary:\n",
      "Average loss: 3.7273\n",
      "Average policy_loss: 2.6715\n",
      "Average value_loss: 1.0558\n",
      "Replay buffer size: 100000\n",
      "Time taken: 917.7s\n",
      "\n",
      "Iteration 297/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 297 summary:\n",
      "Average loss: 3.7219\n",
      "Average policy_loss: 2.6596\n",
      "Average value_loss: 1.0622\n",
      "Replay buffer size: 100000\n",
      "Time taken: 925.6s\n",
      "\n",
      "Iteration 298/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 298 summary:\n",
      "Average loss: 3.7052\n",
      "Average policy_loss: 2.6551\n",
      "Average value_loss: 1.0501\n",
      "Replay buffer size: 100000\n",
      "Time taken: 906.8s\n",
      "\n",
      "Iteration 299/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 299 summary:\n",
      "Average loss: 3.7137\n",
      "Average policy_loss: 2.6517\n",
      "Average value_loss: 1.0620\n",
      "Replay buffer size: 100000\n",
      "Time taken: 915.7s\n",
      "\n",
      "Iteration 300/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 300 summary:\n",
      "Average loss: 3.7285\n",
      "Average policy_loss: 2.6728\n",
      "Average value_loss: 1.0557\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1767.2s\n",
      "\n",
      "Iteration 301/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 301 summary:\n",
      "Average loss: 3.7210\n",
      "Average policy_loss: 2.6551\n",
      "Average value_loss: 1.0659\n",
      "Replay buffer size: 100000\n",
      "Time taken: 916.0s\n",
      "\n",
      "Iteration 302/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 302 summary:\n",
      "Average loss: 3.7223\n",
      "Average policy_loss: 2.6697\n",
      "Average value_loss: 1.0526\n",
      "Replay buffer size: 100000\n",
      "Time taken: 913.0s\n",
      "\n",
      "Iteration 303/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 303 summary:\n",
      "Average loss: 3.7335\n",
      "Average policy_loss: 2.6692\n",
      "Average value_loss: 1.0643\n",
      "Replay buffer size: 100000\n",
      "Time taken: 908.5s\n",
      "\n",
      "Iteration 304/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 304 summary:\n",
      "Average loss: 3.7129\n",
      "Average policy_loss: 2.6528\n",
      "Average value_loss: 1.0602\n",
      "Replay buffer size: 100000\n",
      "Time taken: 841.1s\n",
      "\n",
      "Iteration 305/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 40.00%, Draw rate = 0.00%, Loss rate = 60.00%\n",
      "\n",
      "Iteration 305 summary:\n",
      "Average loss: 3.7233\n",
      "Average policy_loss: 2.6488\n",
      "Average value_loss: 1.0745\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1608.3s\n",
      "\n",
      "Iteration 306/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 306 summary:\n",
      "Average loss: 3.7343\n",
      "Average policy_loss: 2.6629\n",
      "Average value_loss: 1.0713\n",
      "Replay buffer size: 100000\n",
      "Time taken: 818.9s\n",
      "\n",
      "Iteration 307/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 307 summary:\n",
      "Average loss: 3.7269\n",
      "Average policy_loss: 2.6544\n",
      "Average value_loss: 1.0725\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.7s\n",
      "\n",
      "Iteration 308/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 308 summary:\n",
      "Average loss: 3.7070\n",
      "Average policy_loss: 2.6389\n",
      "Average value_loss: 1.0681\n",
      "Replay buffer size: 100000\n",
      "Time taken: 817.1s\n",
      "\n",
      "Iteration 309/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 309 summary:\n",
      "Average loss: 3.7212\n",
      "Average policy_loss: 2.6462\n",
      "Average value_loss: 1.0750\n",
      "Replay buffer size: 100000\n",
      "Time taken: 819.4s\n",
      "\n",
      "Iteration 310/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 15.00%, Draw rate = 0.00%, Loss rate = 85.00%\n",
      "\n",
      "Iteration 310 summary:\n",
      "Average loss: 3.7271\n",
      "Average policy_loss: 2.6481\n",
      "Average value_loss: 1.0790\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1590.1s\n",
      "\n",
      "Iteration 311/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 311 summary:\n",
      "Average loss: 3.7198\n",
      "Average policy_loss: 2.6401\n",
      "Average value_loss: 1.0797\n",
      "Replay buffer size: 100000\n",
      "Time taken: 817.6s\n",
      "\n",
      "Iteration 312/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 312 summary:\n",
      "Average loss: 3.7237\n",
      "Average policy_loss: 2.6454\n",
      "Average value_loss: 1.0784\n",
      "Replay buffer size: 100000\n",
      "Time taken: 820.5s\n",
      "\n",
      "Iteration 313/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 313 summary:\n",
      "Average loss: 3.7710\n",
      "Average policy_loss: 2.6376\n",
      "Average value_loss: 1.1334\n",
      "Replay buffer size: 100000\n",
      "Time taken: 818.9s\n",
      "\n",
      "Iteration 314/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 314 summary:\n",
      "Average loss: 3.8263\n",
      "Average policy_loss: 2.6444\n",
      "Average value_loss: 1.1819\n",
      "Replay buffer size: 100000\n",
      "Time taken: 819.3s\n",
      "\n",
      "Iteration 315/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 315 summary:\n",
      "Average loss: 3.8361\n",
      "Average policy_loss: 2.6533\n",
      "Average value_loss: 1.1828\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1679.9s\n",
      "\n",
      "Iteration 316/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 316 summary:\n",
      "Average loss: 3.8444\n",
      "Average policy_loss: 2.6568\n",
      "Average value_loss: 1.1877\n",
      "Replay buffer size: 100000\n",
      "Time taken: 818.6s\n",
      "\n",
      "Iteration 317/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 317 summary:\n",
      "Average loss: 3.8282\n",
      "Average policy_loss: 2.6408\n",
      "Average value_loss: 1.1874\n",
      "Replay buffer size: 100000\n",
      "Time taken: 818.8s\n",
      "\n",
      "Iteration 318/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 318 summary:\n",
      "Average loss: 3.8481\n",
      "Average policy_loss: 2.6536\n",
      "Average value_loss: 1.1945\n",
      "Replay buffer size: 100000\n",
      "Time taken: 816.0s\n",
      "\n",
      "Iteration 319/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 319 summary:\n",
      "Average loss: 3.8339\n",
      "Average policy_loss: 2.6326\n",
      "Average value_loss: 1.2013\n",
      "Replay buffer size: 100000\n",
      "Time taken: 816.1s\n",
      "\n",
      "Iteration 320/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 320 summary:\n",
      "Average loss: 3.8409\n",
      "Average policy_loss: 2.6420\n",
      "Average value_loss: 1.1990\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1719.9s\n",
      "\n",
      "Iteration 321/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 321 summary:\n",
      "Average loss: 3.8394\n",
      "Average policy_loss: 2.6395\n",
      "Average value_loss: 1.2000\n",
      "Replay buffer size: 100000\n",
      "Time taken: 822.5s\n",
      "\n",
      "Iteration 322/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 322 summary:\n",
      "Average loss: 3.8348\n",
      "Average policy_loss: 2.6371\n",
      "Average value_loss: 1.1978\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.7s\n",
      "\n",
      "Iteration 323/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 323 summary:\n",
      "Average loss: 3.8235\n",
      "Average policy_loss: 2.6187\n",
      "Average value_loss: 1.2048\n",
      "Replay buffer size: 100000\n",
      "Time taken: 814.4s\n",
      "\n",
      "Iteration 324/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 324 summary:\n",
      "Average loss: 3.8455\n",
      "Average policy_loss: 2.6383\n",
      "Average value_loss: 1.2072\n",
      "Replay buffer size: 100000\n",
      "Time taken: 809.3s\n",
      "\n",
      "Iteration 325/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 325 summary:\n",
      "Average loss: 3.8293\n",
      "Average policy_loss: 2.6157\n",
      "Average value_loss: 1.2136\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1718.1s\n",
      "\n",
      "Iteration 326/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 326 summary:\n",
      "Average loss: 3.8152\n",
      "Average policy_loss: 2.6033\n",
      "Average value_loss: 1.2119\n",
      "Replay buffer size: 100000\n",
      "Time taken: 828.3s\n",
      "\n",
      "Iteration 327/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 327 summary:\n",
      "Average loss: 3.8000\n",
      "Average policy_loss: 2.5952\n",
      "Average value_loss: 1.2048\n",
      "Replay buffer size: 100000\n",
      "Time taken: 830.8s\n",
      "\n",
      "Iteration 328/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 328 summary:\n",
      "Average loss: 3.8034\n",
      "Average policy_loss: 2.5843\n",
      "Average value_loss: 1.2191\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.5s\n",
      "\n",
      "Iteration 329/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 329 summary:\n",
      "Average loss: 3.8090\n",
      "Average policy_loss: 2.5902\n",
      "Average value_loss: 1.2188\n",
      "Replay buffer size: 100000\n",
      "Time taken: 827.3s\n",
      "\n",
      "Iteration 330/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 95.00%, Draw rate = 0.00%, Loss rate = 5.00%\n",
      "\n",
      "Iteration 330 summary:\n",
      "Average loss: 3.8095\n",
      "Average policy_loss: 2.5818\n",
      "Average value_loss: 1.2277\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1744.8s\n",
      "\n",
      "Iteration 331/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 331 summary:\n",
      "Average loss: 3.8062\n",
      "Average policy_loss: 2.5805\n",
      "Average value_loss: 1.2258\n",
      "Replay buffer size: 100000\n",
      "Time taken: 817.2s\n",
      "\n",
      "Iteration 332/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 332 summary:\n",
      "Average loss: 3.7996\n",
      "Average policy_loss: 2.5737\n",
      "Average value_loss: 1.2259\n",
      "Replay buffer size: 100000\n",
      "Time taken: 829.4s\n",
      "\n",
      "Iteration 333/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 333 summary:\n",
      "Average loss: 3.7951\n",
      "Average policy_loss: 2.5676\n",
      "Average value_loss: 1.2274\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.4s\n",
      "\n",
      "Iteration 334/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 334 summary:\n",
      "Average loss: 3.8048\n",
      "Average policy_loss: 2.5710\n",
      "Average value_loss: 1.2338\n",
      "Replay buffer size: 100000\n",
      "Time taken: 824.0s\n",
      "\n",
      "Iteration 335/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 90.00%, Draw rate = 0.00%, Loss rate = 10.00%\n",
      "\n",
      "Iteration 335 summary:\n",
      "Average loss: 3.8068\n",
      "Average policy_loss: 2.5744\n",
      "Average value_loss: 1.2324\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1706.2s\n",
      "\n",
      "Iteration 336/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 336 summary:\n",
      "Average loss: 3.7975\n",
      "Average policy_loss: 2.5685\n",
      "Average value_loss: 1.2290\n",
      "Replay buffer size: 100000\n",
      "Time taken: 827.0s\n",
      "\n",
      "Iteration 337/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 337 summary:\n",
      "Average loss: 3.7966\n",
      "Average policy_loss: 2.5700\n",
      "Average value_loss: 1.2266\n",
      "Replay buffer size: 100000\n",
      "Time taken: 827.9s\n",
      "\n",
      "Iteration 338/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 338 summary:\n",
      "Average loss: 3.7830\n",
      "Average policy_loss: 2.5559\n",
      "Average value_loss: 1.2271\n",
      "Replay buffer size: 100000\n",
      "Time taken: 827.7s\n",
      "\n",
      "Iteration 339/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 339 summary:\n",
      "Average loss: 3.7942\n",
      "Average policy_loss: 2.5722\n",
      "Average value_loss: 1.2220\n",
      "Replay buffer size: 100000\n",
      "Time taken: 826.2s\n",
      "\n",
      "Iteration 340/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 100.00%, Draw rate = 0.00%, Loss rate = 0.00%\n",
      "\n",
      "Iteration 340 summary:\n",
      "Average loss: 3.7991\n",
      "Average policy_loss: 2.5627\n",
      "Average value_loss: 1.2363\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1714.4s\n",
      "\n",
      "Iteration 341/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 341 summary:\n",
      "Average loss: 3.7909\n",
      "Average policy_loss: 2.5538\n",
      "Average value_loss: 1.2370\n",
      "Replay buffer size: 100000\n",
      "Time taken: 815.1s\n",
      "\n",
      "Iteration 342/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 342 summary:\n",
      "Average loss: 3.7798\n",
      "Average policy_loss: 2.5461\n",
      "Average value_loss: 1.2337\n",
      "Replay buffer size: 100000\n",
      "Time taken: 831.3s\n",
      "\n",
      "Iteration 343/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 343 summary:\n",
      "Average loss: 3.7835\n",
      "Average policy_loss: 2.5426\n",
      "Average value_loss: 1.2410\n",
      "Replay buffer size: 100000\n",
      "Time taken: 846.9s\n",
      "\n",
      "Iteration 344/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 344 summary:\n",
      "Average loss: 3.8020\n",
      "Average policy_loss: 2.5525\n",
      "Average value_loss: 1.2495\n",
      "Replay buffer size: 100000\n",
      "Time taken: 869.5s\n",
      "\n",
      "Iteration 345/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 345 summary:\n",
      "Average loss: 3.7758\n",
      "Average policy_loss: 2.5400\n",
      "Average value_loss: 1.2358\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1761.3s\n",
      "\n",
      "Iteration 346/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 346 summary:\n",
      "Average loss: 3.7845\n",
      "Average policy_loss: 2.5341\n",
      "Average value_loss: 1.2503\n",
      "Replay buffer size: 100000\n",
      "Time taken: 831.4s\n",
      "\n",
      "Iteration 347/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 347 summary:\n",
      "Average loss: 3.7809\n",
      "Average policy_loss: 2.5352\n",
      "Average value_loss: 1.2457\n",
      "Replay buffer size: 100000\n",
      "Time taken: 862.6s\n",
      "\n",
      "Iteration 348/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 348 summary:\n",
      "Average loss: 3.7834\n",
      "Average policy_loss: 2.5361\n",
      "Average value_loss: 1.2472\n",
      "Replay buffer size: 100000\n",
      "Time taken: 830.5s\n",
      "\n",
      "Iteration 349/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 349 summary:\n",
      "Average loss: 3.7892\n",
      "Average policy_loss: 2.5378\n",
      "Average value_loss: 1.2514\n",
      "Replay buffer size: 100000\n",
      "Time taken: 843.7s\n",
      "\n",
      "Iteration 350/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 85.00%, Draw rate = 0.00%, Loss rate = 15.00%\n",
      "\n",
      "Iteration 350 summary:\n",
      "Average loss: 3.7761\n",
      "Average policy_loss: 2.5243\n",
      "Average value_loss: 1.2518\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1717.5s\n",
      "\n",
      "Iteration 351/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 351 summary:\n",
      "Average loss: 3.7811\n",
      "Average policy_loss: 2.5174\n",
      "Average value_loss: 1.2637\n",
      "Replay buffer size: 100000\n",
      "Time taken: 851.5s\n",
      "\n",
      "Iteration 352/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 352 summary:\n",
      "Average loss: 3.7270\n",
      "Average policy_loss: 2.5186\n",
      "Average value_loss: 1.2084\n",
      "Replay buffer size: 100000\n",
      "Time taken: 871.7s\n",
      "\n",
      "Iteration 353/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 353 summary:\n",
      "Average loss: 3.5244\n",
      "Average policy_loss: 2.5086\n",
      "Average value_loss: 1.0158\n",
      "Replay buffer size: 100000\n",
      "Time taken: 13653.4s\n",
      "\n",
      "Iteration 354/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 354 summary:\n",
      "Average loss: 3.5366\n",
      "Average policy_loss: 2.5162\n",
      "Average value_loss: 1.0204\n",
      "Replay buffer size: 100000\n",
      "Time taken: 917.3s\n",
      "\n",
      "Iteration 355/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 30.00%, Draw rate = 0.00%, Loss rate = 70.00%\n",
      "\n",
      "Iteration 355 summary:\n",
      "Average loss: 3.5407\n",
      "Average policy_loss: 2.5214\n",
      "Average value_loss: 1.0193\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1751.0s\n",
      "\n",
      "Iteration 356/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 356 summary:\n",
      "Average loss: 3.5329\n",
      "Average policy_loss: 2.5130\n",
      "Average value_loss: 1.0199\n",
      "Replay buffer size: 100000\n",
      "Time taken: 916.1s\n",
      "\n",
      "Iteration 357/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 357 summary:\n",
      "Average loss: 3.5231\n",
      "Average policy_loss: 2.5059\n",
      "Average value_loss: 1.0172\n",
      "Replay buffer size: 100000\n",
      "Time taken: 915.4s\n",
      "\n",
      "Iteration 358/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 358 summary:\n",
      "Average loss: 3.5198\n",
      "Average policy_loss: 2.5081\n",
      "Average value_loss: 1.0118\n",
      "Replay buffer size: 100000\n",
      "Time taken: 911.0s\n",
      "\n",
      "Iteration 359/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 359 summary:\n",
      "Average loss: 3.5217\n",
      "Average policy_loss: 2.5101\n",
      "Average value_loss: 1.0116\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1000.6s\n",
      "\n",
      "Iteration 360/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 40.00%, Draw rate = 0.00%, Loss rate = 60.00%\n",
      "\n",
      "Iteration 360 summary:\n",
      "Average loss: 3.5184\n",
      "Average policy_loss: 2.4982\n",
      "Average value_loss: 1.0202\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1618.6s\n",
      "\n",
      "Iteration 361/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 361 summary:\n",
      "Average loss: 3.5259\n",
      "Average policy_loss: 2.4960\n",
      "Average value_loss: 1.0299\n",
      "Replay buffer size: 100000\n",
      "Time taken: 839.1s\n",
      "\n",
      "Iteration 362/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 362 summary:\n",
      "Average loss: 3.5231\n",
      "Average policy_loss: 2.5024\n",
      "Average value_loss: 1.0208\n",
      "Replay buffer size: 100000\n",
      "Time taken: 856.6s\n",
      "\n",
      "Iteration 363/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 363 summary:\n",
      "Average loss: 3.5392\n",
      "Average policy_loss: 2.5189\n",
      "Average value_loss: 1.0203\n",
      "Replay buffer size: 100000\n",
      "Time taken: 855.2s\n",
      "\n",
      "Iteration 364/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 364 summary:\n",
      "Average loss: 3.5390\n",
      "Average policy_loss: 2.5165\n",
      "Average value_loss: 1.0224\n",
      "Replay buffer size: 100000\n",
      "Time taken: 840.6s\n",
      "\n",
      "Iteration 365/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 25.00%, Draw rate = 0.00%, Loss rate = 75.00%\n",
      "\n",
      "Iteration 365 summary:\n",
      "Average loss: 3.5275\n",
      "Average policy_loss: 2.5120\n",
      "Average value_loss: 1.0155\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1640.4s\n",
      "\n",
      "Iteration 366/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 366 summary:\n",
      "Average loss: 3.5463\n",
      "Average policy_loss: 2.5163\n",
      "Average value_loss: 1.0300\n",
      "Replay buffer size: 100000\n",
      "Time taken: 823.2s\n",
      "\n",
      "Iteration 367/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 367 summary:\n",
      "Average loss: 3.5426\n",
      "Average policy_loss: 2.5191\n",
      "Average value_loss: 1.0234\n",
      "Replay buffer size: 100000\n",
      "Time taken: 824.3s\n",
      "\n",
      "Iteration 368/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 368 summary:\n",
      "Average loss: 3.5403\n",
      "Average policy_loss: 2.5174\n",
      "Average value_loss: 1.0229\n",
      "Replay buffer size: 100000\n",
      "Time taken: 814.1s\n",
      "\n",
      "Iteration 369/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 369 summary:\n",
      "Average loss: 3.5430\n",
      "Average policy_loss: 2.5162\n",
      "Average value_loss: 1.0268\n",
      "Replay buffer size: 100000\n",
      "Time taken: 817.1s\n",
      "\n",
      "Iteration 370/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 370 summary:\n",
      "Average loss: 3.5353\n",
      "Average policy_loss: 2.5079\n",
      "Average value_loss: 1.0274\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1576.2s\n",
      "\n",
      "Iteration 371/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 371 summary:\n",
      "Average loss: 3.5291\n",
      "Average policy_loss: 2.5004\n",
      "Average value_loss: 1.0287\n",
      "Replay buffer size: 100000\n",
      "Time taken: 851.9s\n",
      "\n",
      "Iteration 372/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 372 summary:\n",
      "Average loss: 3.5355\n",
      "Average policy_loss: 2.5053\n",
      "Average value_loss: 1.0301\n",
      "Replay buffer size: 100000\n",
      "Time taken: 807.5s\n",
      "\n",
      "Iteration 373/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 373 summary:\n",
      "Average loss: 3.5371\n",
      "Average policy_loss: 2.5054\n",
      "Average value_loss: 1.0317\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.6s\n",
      "\n",
      "Iteration 374/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 374 summary:\n",
      "Average loss: 3.5495\n",
      "Average policy_loss: 2.5155\n",
      "Average value_loss: 1.0339\n",
      "Replay buffer size: 100000\n",
      "Time taken: 809.4s\n",
      "\n",
      "Iteration 375/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 10.00%, Draw rate = 0.00%, Loss rate = 90.00%\n",
      "\n",
      "Iteration 375 summary:\n",
      "Average loss: 3.5466\n",
      "Average policy_loss: 2.5121\n",
      "Average value_loss: 1.0345\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1564.8s\n",
      "\n",
      "Iteration 376/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 376 summary:\n",
      "Average loss: 3.5441\n",
      "Average policy_loss: 2.5095\n",
      "Average value_loss: 1.0346\n",
      "Replay buffer size: 100000\n",
      "Time taken: 810.1s\n",
      "\n",
      "Iteration 377/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 377 summary:\n",
      "Average loss: 3.5440\n",
      "Average policy_loss: 2.4992\n",
      "Average value_loss: 1.0448\n",
      "Replay buffer size: 100000\n",
      "Time taken: 812.4s\n",
      "\n",
      "Iteration 378/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 378 summary:\n",
      "Average loss: 3.5641\n",
      "Average policy_loss: 2.5161\n",
      "Average value_loss: 1.0480\n",
      "Replay buffer size: 100000\n",
      "Time taken: 823.4s\n",
      "\n",
      "Iteration 379/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 379 summary:\n",
      "Average loss: 3.5526\n",
      "Average policy_loss: 2.5101\n",
      "Average value_loss: 1.0424\n",
      "Replay buffer size: 100000\n",
      "Time taken: 868.7s\n",
      "\n",
      "Iteration 380/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 20.00%, Draw rate = 0.00%, Loss rate = 80.00%\n",
      "\n",
      "Iteration 380 summary:\n",
      "Average loss: 3.5445\n",
      "Average policy_loss: 2.4939\n",
      "Average value_loss: 1.0506\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1599.5s\n",
      "\n",
      "Iteration 381/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 381 summary:\n",
      "Average loss: 3.5617\n",
      "Average policy_loss: 2.5121\n",
      "Average value_loss: 1.0495\n",
      "Replay buffer size: 100000\n",
      "Time taken: 828.6s\n",
      "\n",
      "Iteration 382/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 382 summary:\n",
      "Average loss: 3.5572\n",
      "Average policy_loss: 2.5132\n",
      "Average value_loss: 1.0440\n",
      "Replay buffer size: 100000\n",
      "Time taken: 821.9s\n",
      "\n",
      "Iteration 383/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 383 summary:\n",
      "Average loss: 3.5732\n",
      "Average policy_loss: 2.5261\n",
      "Average value_loss: 1.0472\n",
      "Replay buffer size: 100000\n",
      "Time taken: 832.1s\n",
      "\n",
      "Iteration 384/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 384 summary:\n",
      "Average loss: 3.5716\n",
      "Average policy_loss: 2.5135\n",
      "Average value_loss: 1.0581\n",
      "Replay buffer size: 100000\n",
      "Time taken: 833.1s\n",
      "\n",
      "Iteration 385/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 45.00%, Draw rate = 0.00%, Loss rate = 55.00%\n",
      "\n",
      "Iteration 385 summary:\n",
      "Average loss: 3.5806\n",
      "Average policy_loss: 2.5190\n",
      "Average value_loss: 1.0616\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1625.0s\n",
      "\n",
      "Iteration 386/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 386 summary:\n",
      "Average loss: 3.6854\n",
      "Average policy_loss: 2.5151\n",
      "Average value_loss: 1.1703\n",
      "Replay buffer size: 100000\n",
      "Time taken: 850.0s\n",
      "\n",
      "Iteration 387/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 387 summary:\n",
      "Average loss: 3.7219\n",
      "Average policy_loss: 2.5080\n",
      "Average value_loss: 1.2139\n",
      "Replay buffer size: 100000\n",
      "Time taken: 837.3s\n",
      "\n",
      "Iteration 388/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 388 summary:\n",
      "Average loss: 3.6883\n",
      "Average policy_loss: 2.5073\n",
      "Average value_loss: 1.1810\n",
      "Replay buffer size: 100000\n",
      "Time taken: 842.0s\n",
      "\n",
      "Iteration 389/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 389 summary:\n",
      "Average loss: 3.5849\n",
      "Average policy_loss: 2.5200\n",
      "Average value_loss: 1.0649\n",
      "Replay buffer size: 100000\n",
      "Time taken: 832.5s\n",
      "\n",
      "Iteration 390/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 390 summary:\n",
      "Average loss: 3.5764\n",
      "Average policy_loss: 2.5137\n",
      "Average value_loss: 1.0627\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1622.7s\n",
      "\n",
      "Iteration 391/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 391 summary:\n",
      "Average loss: 3.5733\n",
      "Average policy_loss: 2.5172\n",
      "Average value_loss: 1.0562\n",
      "Replay buffer size: 100000\n",
      "Time taken: 844.3s\n",
      "\n",
      "Iteration 392/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 392 summary:\n",
      "Average loss: 3.5912\n",
      "Average policy_loss: 2.5161\n",
      "Average value_loss: 1.0750\n",
      "Replay buffer size: 100000\n",
      "Time taken: 830.3s\n",
      "\n",
      "Iteration 393/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 393 summary:\n",
      "Average loss: 3.6038\n",
      "Average policy_loss: 2.5329\n",
      "Average value_loss: 1.0709\n",
      "Replay buffer size: 100000\n",
      "Time taken: 826.6s\n",
      "\n",
      "Iteration 394/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 394 summary:\n",
      "Average loss: 3.5872\n",
      "Average policy_loss: 2.5138\n",
      "Average value_loss: 1.0734\n",
      "Replay buffer size: 100000\n",
      "Time taken: 825.3s\n",
      "\n",
      "Iteration 395/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 395 summary:\n",
      "Average loss: 3.5939\n",
      "Average policy_loss: 2.5220\n",
      "Average value_loss: 1.0719\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1582.3s\n",
      "\n",
      "Iteration 396/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 396 summary:\n",
      "Average loss: 3.5914\n",
      "Average policy_loss: 2.5210\n",
      "Average value_loss: 1.0704\n",
      "Replay buffer size: 100000\n",
      "Time taken: 850.7s\n",
      "\n",
      "Iteration 397/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 397 summary:\n",
      "Average loss: 3.6148\n",
      "Average policy_loss: 2.5302\n",
      "Average value_loss: 1.0847\n",
      "Replay buffer size: 100000\n",
      "Time taken: 828.8s\n",
      "\n",
      "Iteration 398/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 398 summary:\n",
      "Average loss: 3.6055\n",
      "Average policy_loss: 2.5188\n",
      "Average value_loss: 1.0867\n",
      "Replay buffer size: 100000\n",
      "Time taken: 827.7s\n",
      "\n",
      "Iteration 399/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 399 summary:\n",
      "Average loss: 3.6030\n",
      "Average policy_loss: 2.5159\n",
      "Average value_loss: 1.0871\n",
      "Replay buffer size: 100000\n",
      "Time taken: 845.2s\n",
      "\n",
      "Iteration 400/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 45.00%, Draw rate = 0.00%, Loss rate = 55.00%\n",
      "\n",
      "Iteration 400 summary:\n",
      "Average loss: 3.6073\n",
      "Average policy_loss: 2.5306\n",
      "Average value_loss: 1.0766\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1597.6s\n",
      "\n",
      "Iteration 401/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 401 summary:\n",
      "Average loss: 3.6264\n",
      "Average policy_loss: 2.5343\n",
      "Average value_loss: 1.0920\n",
      "Replay buffer size: 100000\n",
      "Time taken: 835.4s\n",
      "\n",
      "Iteration 402/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 402 summary:\n",
      "Average loss: 3.6132\n",
      "Average policy_loss: 2.5220\n",
      "Average value_loss: 1.0912\n",
      "Replay buffer size: 100000\n",
      "Time taken: 852.6s\n",
      "\n",
      "Iteration 403/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 403 summary:\n",
      "Average loss: 3.6181\n",
      "Average policy_loss: 2.5271\n",
      "Average value_loss: 1.0910\n",
      "Replay buffer size: 100000\n",
      "Time taken: 842.7s\n",
      "\n",
      "Iteration 404/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 404 summary:\n",
      "Average loss: 3.6329\n",
      "Average policy_loss: 2.5302\n",
      "Average value_loss: 1.1027\n",
      "Replay buffer size: 100000\n",
      "Time taken: 837.7s\n",
      "\n",
      "Iteration 405/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 25.00%, Draw rate = 0.00%, Loss rate = 75.00%\n",
      "\n",
      "Iteration 405 summary:\n",
      "Average loss: 3.6255\n",
      "Average policy_loss: 2.5346\n",
      "Average value_loss: 1.0909\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1619.4s\n",
      "\n",
      "Iteration 406/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 406 summary:\n",
      "Average loss: 3.6281\n",
      "Average policy_loss: 2.5336\n",
      "Average value_loss: 1.0944\n",
      "Replay buffer size: 100000\n",
      "Time taken: 833.6s\n",
      "\n",
      "Iteration 407/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 407 summary:\n",
      "Average loss: 3.6336\n",
      "Average policy_loss: 2.5328\n",
      "Average value_loss: 1.1007\n",
      "Replay buffer size: 100000\n",
      "Time taken: 833.5s\n",
      "\n",
      "Iteration 408/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 408 summary:\n",
      "Average loss: 3.6301\n",
      "Average policy_loss: 2.5234\n",
      "Average value_loss: 1.1067\n",
      "Replay buffer size: 100000\n",
      "Time taken: 861.9s\n",
      "\n",
      "Iteration 409/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 409 summary:\n",
      "Average loss: 3.6361\n",
      "Average policy_loss: 2.5313\n",
      "Average value_loss: 1.1048\n",
      "Replay buffer size: 100000\n",
      "Time taken: 844.8s\n",
      "\n",
      "Iteration 410/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 35.00%, Draw rate = 0.00%, Loss rate = 65.00%\n",
      "\n",
      "Iteration 410 summary:\n",
      "Average loss: 3.6288\n",
      "Average policy_loss: 2.5248\n",
      "Average value_loss: 1.1040\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1604.4s\n",
      "\n",
      "Iteration 411/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 411 summary:\n",
      "Average loss: 3.6485\n",
      "Average policy_loss: 2.5449\n",
      "Average value_loss: 1.1036\n",
      "Replay buffer size: 100000\n",
      "Time taken: 836.1s\n",
      "\n",
      "Iteration 412/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 412 summary:\n",
      "Average loss: 3.6430\n",
      "Average policy_loss: 2.5339\n",
      "Average value_loss: 1.1091\n",
      "Replay buffer size: 100000\n",
      "Time taken: 824.1s\n",
      "\n",
      "Iteration 413/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 413 summary:\n",
      "Average loss: 3.6593\n",
      "Average policy_loss: 2.5414\n",
      "Average value_loss: 1.1180\n",
      "Replay buffer size: 100000\n",
      "Time taken: 850.0s\n",
      "\n",
      "Iteration 414/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 414 summary:\n",
      "Average loss: 3.6469\n",
      "Average policy_loss: 2.5310\n",
      "Average value_loss: 1.1159\n",
      "Replay buffer size: 100000\n",
      "Time taken: 828.5s\n",
      "\n",
      "Iteration 415/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n",
      "\n",
      "Evaluation results:\n",
      "RandomAgent: Win rate = 30.00%, Draw rate = 0.00%, Loss rate = 70.00%\n",
      "\n",
      "Iteration 415 summary:\n",
      "Average loss: 3.6535\n",
      "Average policy_loss: 2.5353\n",
      "Average value_loss: 1.1183\n",
      "Replay buffer size: 100000\n",
      "Time taken: 1584.2s\n",
      "\n",
      "Iteration 416/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 416 summary:\n",
      "Average loss: 3.6640\n",
      "Average policy_loss: 2.5367\n",
      "Average value_loss: 1.1274\n",
      "Replay buffer size: 100000\n",
      "Time taken: 832.4s\n",
      "\n",
      "Iteration 417/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 417 summary:\n",
      "Average loss: 3.6657\n",
      "Average policy_loss: 2.5567\n",
      "Average value_loss: 1.1090\n",
      "Replay buffer size: 100000\n",
      "Time taken: 838.9s\n",
      "\n",
      "Iteration 418/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 418 summary:\n",
      "Average loss: 3.6734\n",
      "Average policy_loss: 2.5414\n",
      "Average value_loss: 1.1321\n",
      "Replay buffer size: 100000\n",
      "Time taken: 927.4s\n",
      "\n",
      "Iteration 419/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Iteration 419 summary:\n",
      "Average loss: 3.6634\n",
      "Average policy_loss: 2.5469\n",
      "Average value_loss: 1.1165\n",
      "Replay buffer size: 100000\n",
      "Time taken: 834.9s\n",
      "\n",
      "Iteration 420/650\n",
      "Self-play phase...\n",
      "Playing game 10/10\n",
      "Generated 610 new positions\n",
      "Training phase...\n",
      "Current learning rate: 0.005000\n",
      "\n",
      "Evaluating against opponents...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform training\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdots_and_boxes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m----> 5\u001b[0m model_interface \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_rows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_cols\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_model_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_replay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_replay_buffer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_replay_buffer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/applications/dots_and_boxes/train.py:178\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_rows, num_cols, optimizer_type, optimizer_params, lr_scheduler_type, lr_scheduler_params, training_method, model_type, model_params, device, model_name, load_model, load_model_params, load_replay_buffer, load_replay_buffer_params, wandb_run, wandb_watch_params, trainer_params, training_params)\u001b[0m\n\u001b[1;32m    170\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m AlphaZeroTrainer(\n\u001b[1;32m    171\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_interface,\n\u001b[1;32m    172\u001b[0m         tensor_mapping\u001b[38;5;241m=\u001b[39mmodel_tensor_mapping,\n\u001b[1;32m    173\u001b[0m         replay_buffer\u001b[38;5;241m=\u001b[39mreplay_buffer,\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_params\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDotsAndBoxesGameState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_against_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopponents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_search_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtree_search_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_search_eval_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtree_search_eval_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_iterations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgames_per_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgames_per_iteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_iteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_simulations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_frequency\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstart_iteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    197\u001b[0m     supervised_training_loop(\n\u001b[1;32m    198\u001b[0m         model_interface\u001b[38;5;241m=\u001b[39mmodel_interface,\n\u001b[1;32m    199\u001b[0m         tensor_mapping\u001b[38;5;241m=\u001b[39mmodel_tensor_mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m         start_at\u001b[38;5;241m=\u001b[39mtraining_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_at\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/trainer.py:234\u001b[0m, in \u001b[0;36mTreeSearchTrainer.train\u001b[0;34m(self, initial_state, tree_search_params, optimizer, num_iterations, games_per_iteration, batch_size, steps_per_iteration, num_simulations, checkpoint_frequency, checkpoints_folder, evaluate_against_agents, eval_frequency, tree_search_eval_params, verbose, wandb_run, model_name, start_iteration, lr_scheduler)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating against opponents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 234\u001b[0m eval_stats \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tree_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtree_search_eval_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree_search_eval_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtree_search_params\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_opponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_against_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_state\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m log_dict\u001b[38;5;241m.\u001b[39mupdate(eval_stats)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/benchmark.py:56\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(create_agent, create_opponents, initial_state, num_games)\u001b[0m\n\u001b[1;32m     54\u001b[0m is_first_player_turn \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcurrent_player \u001b[38;5;241m==\u001b[39m initial_player\n\u001b[1;32m     55\u001b[0m current_agent \u001b[38;5;241m=\u001b[39m agent \u001b[38;5;28;01mif\u001b[39;00m agent_plays_first \u001b[38;5;241m==\u001b[39m is_first_player_turn \u001b[38;5;28;01melse\u001b[39;00m opponent_agent\n\u001b[0;32m---> 56\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Apply move\u001b[39;00m\n\u001b[1;32m     59\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_action(action)\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/tree_search.py:88\u001b[0m, in \u001b[0;36mTreeSearch.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m     node\u001b[38;5;241m.\u001b[39mexpand(state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dict)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Backpropagation, including the leaf node\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(path):\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/implementations/AlphaZero.py:122\u001b[0m, in \u001b[0;36mAlphaZero.evaluate\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, node\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_reward(node\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcurrent_player), node\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcurrent_player\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Get policy and value from neural network\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy, value, node\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcurrent_player\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/model_interface.py:34\u001b[0m, in \u001b[0;36mModelInterface.predict\u001b[0;34m(self, tensor_mapping, state)\u001b[0m\n\u001b[1;32m     32\u001b[0m encoded_state \u001b[38;5;241m=\u001b[39m tensor_mapping\u001b[38;5;241m.\u001b[39mencode_states([state], device)\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(encoded_state)\n\u001b[0;32m---> 34\u001b[0m decoded_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/applications/dots_and_boxes/encoder.py:61\u001b[0m, in \u001b[0;36mDABBaseTensorMapping.decode_outputs\u001b[0;34m(outputs, states)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, legal_actions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(legal_actions_list):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnum_rows\u001b[38;5;241m*\u001b[39mnum_cols\u001b[38;5;241m+\u001b[39mnum_rows\u001b[38;5;241m+\u001b[39mnum_cols):  \u001b[38;5;66;03m#number of edges in the game\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m         prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert to float\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         row, col \u001b[38;5;241m=\u001b[39m idx_to_edge(idx, num_rows, num_cols)\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (row, col) \u001b[38;5;129;01min\u001b[39;00m legal_actions:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "\n",
    "from applications.dots_and_boxes.train import train\n",
    "\n",
    "model_interface = train(\n",
    "    num_rows = model_params[\"num_rows\"],\n",
    "    num_cols = model_params[\"num_cols\"],\n",
    "    model_type=model_type,\n",
    "    model_params=model_params,\n",
    "    device=device,\n",
    "    model_name=model_name,\n",
    "    optimizer_type=optimizer_type,\n",
    "    optimizer_params=optimizer_params,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    lr_scheduler_params=lr_scheduler_params,\n",
    "    training_method=training_method,\n",
    "    trainer_params=trainer_params,\n",
    "    training_params=training_params,\n",
    "    load_model=load_model,\n",
    "    load_model_params=load_model_params,\n",
    "    load_replay_buffer=load_replay_buffer,\n",
    "    load_replay_buffer_params=load_replay_buffer_params,\n",
    "    wandb_run=run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'finish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'finish'"
     ]
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 816,130\n",
      "Trainable parameters: 816,130 (100.00%)\n",
      "Non-trainable parameters: 0 (0.00%)\n",
      "\n",
      "Parameters by layer:\n",
      "pos_embedding: 10,880 parameters\n",
      "input_embedding.weight: 896 parameters\n",
      "transformer_blocks.0.self_attn.in_proj_weight: 49,152 parameters\n",
      "transformer_blocks.0.self_attn.in_proj_bias: 384 parameters\n",
      "transformer_blocks.0.self_attn.out_proj.weight: 16,384 parameters\n",
      "transformer_blocks.0.self_attn.out_proj.bias: 128 parameters\n",
      "transformer_blocks.0.linear1.weight: 65,536 parameters\n",
      "transformer_blocks.0.linear1.bias: 512 parameters\n",
      "transformer_blocks.0.linear2.weight: 65,536 parameters\n",
      "transformer_blocks.0.linear2.bias: 128 parameters\n",
      "transformer_blocks.0.norm1.weight: 128 parameters\n",
      "transformer_blocks.0.norm1.bias: 128 parameters\n",
      "transformer_blocks.0.norm2.weight: 128 parameters\n",
      "transformer_blocks.0.norm2.bias: 128 parameters\n",
      "transformer_blocks.1.self_attn.in_proj_weight: 49,152 parameters\n",
      "transformer_blocks.1.self_attn.in_proj_bias: 384 parameters\n",
      "transformer_blocks.1.self_attn.out_proj.weight: 16,384 parameters\n",
      "transformer_blocks.1.self_attn.out_proj.bias: 128 parameters\n",
      "transformer_blocks.1.linear1.weight: 65,536 parameters\n",
      "transformer_blocks.1.linear1.bias: 512 parameters\n",
      "transformer_blocks.1.linear2.weight: 65,536 parameters\n",
      "transformer_blocks.1.linear2.bias: 128 parameters\n",
      "transformer_blocks.1.norm1.weight: 128 parameters\n",
      "transformer_blocks.1.norm1.bias: 128 parameters\n",
      "transformer_blocks.1.norm2.weight: 128 parameters\n",
      "transformer_blocks.1.norm2.bias: 128 parameters\n",
      "transformer_blocks.2.self_attn.in_proj_weight: 49,152 parameters\n",
      "transformer_blocks.2.self_attn.in_proj_bias: 384 parameters\n",
      "transformer_blocks.2.self_attn.out_proj.weight: 16,384 parameters\n",
      "transformer_blocks.2.self_attn.out_proj.bias: 128 parameters\n",
      "transformer_blocks.2.linear1.weight: 65,536 parameters\n",
      "transformer_blocks.2.linear1.bias: 512 parameters\n",
      "transformer_blocks.2.linear2.weight: 65,536 parameters\n",
      "transformer_blocks.2.linear2.bias: 128 parameters\n",
      "transformer_blocks.2.norm1.weight: 128 parameters\n",
      "transformer_blocks.2.norm1.bias: 128 parameters\n",
      "transformer_blocks.2.norm2.weight: 128 parameters\n",
      "transformer_blocks.2.norm2.bias: 128 parameters\n",
      "transformer_blocks.3.self_attn.in_proj_weight: 49,152 parameters\n",
      "transformer_blocks.3.self_attn.in_proj_bias: 384 parameters\n",
      "transformer_blocks.3.self_attn.out_proj.weight: 16,384 parameters\n",
      "transformer_blocks.3.self_attn.out_proj.bias: 128 parameters\n",
      "transformer_blocks.3.linear1.weight: 65,536 parameters\n",
      "transformer_blocks.3.linear1.bias: 512 parameters\n",
      "transformer_blocks.3.linear2.weight: 65,536 parameters\n",
      "transformer_blocks.3.linear2.bias: 128 parameters\n",
      "transformer_blocks.3.norm1.weight: 128 parameters\n",
      "transformer_blocks.3.norm1.bias: 128 parameters\n",
      "transformer_blocks.3.norm2.weight: 128 parameters\n",
      "transformer_blocks.3.norm2.bias: 128 parameters\n",
      "final_norm.weight: 128 parameters\n",
      "final_norm.bias: 128 parameters\n",
      "policy_head.weight: 128 parameters\n",
      "policy_head.bias: 1 parameters\n",
      "value_head.weight: 10,880 parameters\n",
      "value_head.bias: 1 parameters\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the total number of parameters in a PyTorch model,\n",
    "    with a breakdown of trainable vs non-trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,} ({non_trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    # Optional: Print parameters by layer\n",
    "    print(\"\\nParameters by layer:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel():,} parameters\")\n",
    "\n",
    "# Example usage\n",
    "print_model_parameters(model_interface.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots-and-boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
