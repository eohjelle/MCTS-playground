{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes\n"
     ]
    }
   ],
   "source": [
    "# Change directory to the root of the project\n",
    "import os \n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from applications.dots_and_boxes.NNmodels.transformer import TransformerInitParams\n",
    "import torch\n",
    "\n",
    "# Initialize parameters\n",
    "\n",
    "## Model parameters\n",
    "model_type = 'transformer'\n",
    "model_params: TransformerInitParams = {\n",
    "    'num_rows': 3,\n",
    "    'num_cols': 2,\n",
    "    'embed_dim': 128,\n",
    "    'feedforward_dim': 512,\n",
    "    'num_heads': 4,\n",
    "    'attention_layers': 4\n",
    "}\n",
    "device = torch.device('mps')\n",
    "model_name = 'dots_and_boxes_transformer'\n",
    "\n",
    "## Initialize new model\n",
    "load_model = None\n",
    "load_model_params = {}\n",
    "\n",
    "## Optimizer parameters\n",
    "optimizer_type = 'adam'\n",
    "optimizer_params = {\n",
    "    'lr': 1e-2,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'eps': 1e-8,\n",
    "    'weight_decay': 1e-4,\n",
    "    'amsgrad': False\n",
    "}\n",
    "\n",
    "## Learning scheduler parameters\n",
    "lr_scheduler_type = 'plateau'\n",
    "lr_scheduler_params = {\n",
    "    'factor': 0.5,\n",
    "    'patience': 10,\n",
    "    'cooldown': 10,\n",
    "    'min_lr': 1e-6\n",
    "}\n",
    "\n",
    "## Training parameters\n",
    "training_method = 'supervised'\n",
    "trainer_params = {}\n",
    "training_params = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 256,\n",
    "    'eval_freq': 25,\n",
    "    'checkpoint_freq': 50,\n",
    "    'mask_illegal_moves': False,\n",
    "    'mask_value': -20.0, # Doesn't matter when mask_illegal_moves is False\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    'start_at': 1\n",
    "}\n",
    "\n",
    "## Load replay buffer from wandb\n",
    "load_replay_buffer = 'from_file'\n",
    "load_replay_buffer_params = {\n",
    "    'path': f'applications/dots_and_boxes/training_data/dots_and_boxes_{model_params[\"num_rows\"]}x{model_params[\"num_cols\"]}_DABSimpleTensorMapping_minimax.pkl',\n",
    "    'device': device\n",
    "}\n",
    "# load_replay_buffer = 'from_wandb'\n",
    "# load_replay_buffer_params = {\n",
    "#     'project': 'AlphaZero-DotsAndBoxes',\n",
    "#     'artifact_name': f'dots_and_boxes_{model_params[\"num_rows\"]}x{model_params[\"num_cols\"]}_SimpleTensorMapping_minimax',\n",
    "#     'artifact_version': 'latest'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meohjelle\u001b[0m (\u001b[33meigenway\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/eohjelle/Documents/2025-dots-and-boxes/dots-and-boxes/wandb/run-20250320_153746-tynhykdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/tynhykdi' target=\"_blank\">Transformer 3</a></strong> to <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/tynhykdi' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/tynhykdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb run\n",
    "import wandb\n",
    "\n",
    "run_name = 'Transformer 3'\n",
    "notes = 'New transformer model, supervised training on 3 x 2 board. Comparable parameters to Linear Attention Transformer 3.'\n",
    "\n",
    "config = {\n",
    "    'model_type': model_type,\n",
    "    'model_params': model_params,\n",
    "    'optimizer_type': optimizer_type,\n",
    "    'optimizer_params': optimizer_params,\n",
    "    'lr_scheduler_type': lr_scheduler_type,\n",
    "    'lr_scheduler_params': lr_scheduler_params,\n",
    "    'training_method': training_method,\n",
    "    'trainer_params': trainer_params,\n",
    "    'training_params': training_params\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project='AlphaZero-DotsAndBoxes',\n",
    "    name=run_name,\n",
    "    config=config,\n",
    "    notes=notes,\n",
    "    group=f'{training_method} training on {model_params[\"num_rows\"]}x{model_params[\"num_cols\"]} board'\n",
    ")\n",
    "# run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DotsAndBoxesTransformer.__init__() got an unexpected keyword argument 'ff_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform training\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdots_and_boxes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m----> 5\u001b[0m model_interface \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_rows\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_cols\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_model_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_model_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_replay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_replay_buffer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_replay_buffer_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/applications/dots_and_boxes/train.py:87\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_rows, num_cols, optimizer_type, optimizer_params, lr_scheduler_type, lr_scheduler_params, training_method, model_type, model_params, device, model_name, load_model, load_model_params, load_replay_buffer, load_replay_buffer_params, wandb_run, wandb_watch_params, trainer_params, training_params)\u001b[0m\n\u001b[1;32m     78\u001b[0m     model_interface \u001b[38;5;241m=\u001b[39m ModelInterface\u001b[38;5;241m.\u001b[39mfrom_wandb(\n\u001b[1;32m     79\u001b[0m         model_architecture\u001b[38;5;241m=\u001b[39mmodel_architecture,\n\u001b[1;32m     80\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m         model_version\u001b[38;5;241m=\u001b[39mload_model_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_version\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     model_interface \u001b[38;5;241m=\u001b[39m \u001b[43mModelInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_architecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_architecture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid load_model value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/2025-dots-and-boxes/dots-and-boxes/core/model_interface.py:23\u001b[0m, in \u001b[0;36mModelInterface.__init__\u001b[0;34m(self, model_architecture, init_params, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_architecture: Type[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule], init_params: ModelInitParams, device: torch\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mTypeError\u001b[0m: DotsAndBoxesTransformer.__init__() got an unexpected keyword argument 'ff_dim'"
     ]
    }
   ],
   "source": [
    "# Perform training\n",
    "\n",
    "from applications.dots_and_boxes.train import train\n",
    "\n",
    "model_interface = train(\n",
    "    num_rows = model_params[\"num_rows\"],\n",
    "    num_cols = model_params[\"num_cols\"],\n",
    "    model_type=model_type,\n",
    "    model_params=model_params,\n",
    "    device=device,\n",
    "    model_name=model_name,\n",
    "    optimizer_type=optimizer_type,\n",
    "    optimizer_params=optimizer_params,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    lr_scheduler_params=lr_scheduler_params,\n",
    "    training_method=training_method,\n",
    "    trainer_params=trainer_params,\n",
    "    training_params=training_params,\n",
    "    load_model=load_model,\n",
    "    load_model_params=load_model_params,\n",
    "    load_replay_buffer=load_replay_buffer,\n",
    "    load_replay_buffer_params=load_replay_buffer_params,\n",
    "    wandb_run=run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Minimax_draw_rate</td><td>█████████████▃▂████████▇██▁█████████████</td></tr><tr><td>Minimax_loss_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▁▁▁▁▂▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Minimax_score</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▆█▅▅▅▅▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>Minimax_win_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▄█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>RandomAgent_draw_rate</td><td>▅▄█▇▄▅▃▄▂▄▄▄▂▂▄▄▅▄▄▅▇▅▃▃▅▃▅▃▃▁▃▃▃▄▃▃▃▃▂▂</td></tr><tr><td>RandomAgent_loss_rate</td><td>▃▆▃▃▆▆▂▂▃▂▃▂▂▃▁▃█▃▁▃▁▃▂▃▃▂▂▄▁▂▃▂▃▃▂▃▁▂▃▃</td></tr><tr><td>RandomAgent_score</td><td>▄▃▁▂▃▂▇▆▇▆▅▅▇▆▆▄▁▄▆▄▃▃▆▅▄▆▅▅▇█▆▆▅▅▆▆▇▇▇▆</td></tr><tr><td>RandomAgent_win_rate</td><td>▄▄▁▂▄▃▆▅▇▅▅▅▇▇▅▅▂▅▆▄▃▃▆▅▄▆▄▅▆█▆▆▆▅▆▆▇▆▇▆</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>███████████████▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▂▂▂▂▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_policy_loss</td><td>█████▇▇▄▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_value_loss</td><td>█▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▅▆▆▄▄▄▄▄▄▄▄▄▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_policy_loss</td><td>█▅▅▄▄▄▄▄▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_value_loss</td><td>█▇█▇▇▇▆▆▆▅▆▅▆▆▃▂▃▃▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Minimax_draw_rate</td><td>0.5</td></tr><tr><td>Minimax_loss_rate</td><td>0.5</td></tr><tr><td>Minimax_score</td><td>-0.5</td></tr><tr><td>Minimax_win_rate</td><td>0</td></tr><tr><td>RandomAgent_draw_rate</td><td>0.09</td></tr><tr><td>RandomAgent_loss_rate</td><td>0.03</td></tr><tr><td>RandomAgent_score</td><td>0.85</td></tr><tr><td>RandomAgent_win_rate</td><td>0.88</td></tr><tr><td>epoch</td><td>1000</td></tr><tr><td>learning_rate</td><td>0.00031</td></tr><tr><td>train_loss</td><td>1.29524</td></tr><tr><td>train_policy_loss</td><td>1.12166</td></tr><tr><td>train_value_loss</td><td>0.17359</td></tr><tr><td>val_loss</td><td>1.29954</td></tr><tr><td>val_policy_loss</td><td>1.0901</td></tr><tr><td>val_value_loss</td><td>0.20944</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Linear Attention Transformer</strong> at: <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/xm5czpvo' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes/runs/xm5czpvo</a><br> View project at: <a href='https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes' target=\"_blank\">https://wandb.ai/eigenway/AlphaZero-DotsAndBoxes</a><br>Synced 5 W&B file(s), 1 media file(s), 232 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250320_102631-xm5czpvo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 13,602\n",
      "Trainable parameters: 13,602 (100.00%)\n",
      "Non-trainable parameters: 0 (0.00%)\n",
      "\n",
      "Parameters by layer:\n",
      "pos_embedding: 384 parameters\n",
      "input_embedding.weight: 32 parameters\n",
      "transformer_blocks.0.norm1.weight: 32 parameters\n",
      "transformer_blocks.0.norm1.bias: 32 parameters\n",
      "transformer_blocks.0.attn.q_emb.weight: 1,024 parameters\n",
      "transformer_blocks.0.attn.q_emb.bias: 32 parameters\n",
      "transformer_blocks.0.attn.k_emb.weight: 1,024 parameters\n",
      "transformer_blocks.0.attn.k_emb.bias: 32 parameters\n",
      "transformer_blocks.0.attn.v_emb.weight: 1,024 parameters\n",
      "transformer_blocks.0.attn.v_emb.bias: 32 parameters\n",
      "transformer_blocks.0.attn.out_emb.weight: 1,024 parameters\n",
      "transformer_blocks.0.attn.out_emb.bias: 32 parameters\n",
      "transformer_blocks.0.norm2.weight: 32 parameters\n",
      "transformer_blocks.0.norm2.bias: 32 parameters\n",
      "transformer_blocks.0.ff.0.weight: 4,096 parameters\n",
      "transformer_blocks.0.ff.0.bias: 128 parameters\n",
      "transformer_blocks.0.ff.2.weight: 4,096 parameters\n",
      "transformer_blocks.0.ff.2.bias: 32 parameters\n",
      "final_norm.weight: 32 parameters\n",
      "final_norm.bias: 32 parameters\n",
      "policy_head.weight: 32 parameters\n",
      "policy_head.bias: 1 parameters\n",
      "value_head.weight: 384 parameters\n",
      "value_head.bias: 1 parameters\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the total number of parameters in a PyTorch model,\n",
    "    with a breakdown of trainable vs non-trainable parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params:.2%})\")\n",
    "    print(f\"Non-trainable parameters: {non_trainable_params:,} ({non_trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    # Optional: Print parameters by layer\n",
    "    print(\"\\nParameters by layer:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel():,} parameters\")\n",
    "\n",
    "# Example usage\n",
    "print_model_parameters(model_interface.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dots-and-boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
